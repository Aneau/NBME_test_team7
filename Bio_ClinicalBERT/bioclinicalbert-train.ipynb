{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfb5f02",
   "metadata": {
    "papermill": {
     "duration": 0.0195,
     "end_time": "2025-05-28T07:18:31.403615",
     "exception": false,
     "start_time": "2025-05-28T07:18:31.384115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# About this notebook\n",
    "- Deberta-base starter code\n",
    "- pip wheels is [here](https://www.kaggle.com/yasufuminakama/nbme-pip-wheels)\n",
    "- Inference notebook is [here](https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-inference)\n",
    "\n",
    "If this notebook is helpful, feel free to upvote :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f19c2f0",
   "metadata": {
    "papermill": {
     "duration": 0.018022,
     "end_time": "2025-05-28T07:18:31.440474",
     "exception": false,
     "start_time": "2025-05-28T07:18:31.422452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c50b869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:31.485088Z",
     "iopub.status.busy": "2025-05-28T07:18:31.484387Z",
     "iopub.status.idle": "2025-05-28T07:18:31.488692Z",
     "shell.execute_reply": "2025-05-28T07:18:31.489160Z",
     "shell.execute_reply.started": "2025-05-28T03:31:41.636784Z"
    },
    "papermill": {
     "duration": 0.030676,
     "end_time": "2025-05-28T07:18:31.489436",
     "exception": false,
     "start_time": "2025-05-28T07:18:31.458760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1891169",
   "metadata": {
    "papermill": {
     "duration": 0.018351,
     "end_time": "2025-05-28T07:18:31.526073",
     "exception": false,
     "start_time": "2025-05-28T07:18:31.507722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "476e91f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:31.568867Z",
     "iopub.status.busy": "2025-05-28T07:18:31.564713Z",
     "iopub.status.idle": "2025-05-28T07:18:31.571216Z",
     "shell.execute_reply": "2025-05-28T07:18:31.570699Z"
    },
    "papermill": {
     "duration": 0.027371,
     "end_time": "2025-05-28T07:18:31.571340",
     "exception": false,
     "start_time": "2025-05-28T07:18:31.543969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    wandb=True\n",
    "    competition='NBME'\n",
    "    _wandb_kernel='nakama'\n",
    "    debug=False\n",
    "    apex=True\n",
    "    print_freq=100\n",
    "    num_workers=4\n",
    "    model=\"/kaggle/input/bio-clinicalbert\"\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=3\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=4\n",
    "    fc_dropout=0.2\n",
    "    max_len=256\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    trn_fold=[0,1,2,3,4]\n",
    "    train=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861dfcdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:31.614673Z",
     "iopub.status.busy": "2025-05-28T07:18:31.614061Z",
     "iopub.status.idle": "2025-05-28T07:18:40.095909Z",
     "shell.execute_reply": "2025-05-28T07:18:40.095242Z",
     "shell.execute_reply.started": "2025-05-28T03:31:41.684065Z"
    },
    "papermill": {
     "duration": 8.506447,
     "end_time": "2025-05-28T07:18:40.096064",
     "exception": false,
     "start_time": "2025-05-28T07:18:31.589617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \n",
      "Get your W&B access token from here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.19.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/anony-moose-234118703258025913/NBME-Public/runs/2ju4kja4?apiKey=52bf7cd93ad351d276f3e0fcdc3cd6b4a6347127\" target=\"_blank\">/kaggle/input/bio-clinicalbert</a></strong> to <a href=\"https://wandb.ai/anony-moose-234118703258025913/NBME-Public?apiKey=52bf7cd93ad351d276f3e0fcdc3cd6b4a6347127\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# wandb\n",
    "# ====================================================\n",
    "if CFG.wandb:\n",
    "    \n",
    "    import wandb\n",
    "\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "        wandb.login(key=secret_value_0)\n",
    "        anony = None\n",
    "    except:\n",
    "        anony = \"must\"\n",
    "        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
    "\n",
    "\n",
    "    def class2dict(f):\n",
    "        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "    run = wandb.init(project='NBME-Public', \n",
    "                     name=CFG.model,\n",
    "                     config=class2dict(CFG),\n",
    "                     group=CFG.model,\n",
    "                     job_type=\"train\",\n",
    "                     anonymous=anony)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783d0eec",
   "metadata": {
    "papermill": {
     "duration": 0.018757,
     "end_time": "2025-05-28T07:18:40.135467",
     "exception": false,
     "start_time": "2025-05-28T07:18:40.116710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9752a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:40.189161Z",
     "iopub.status.busy": "2025-05-28T07:18:40.184471Z",
     "iopub.status.idle": "2025-05-28T07:18:56.968522Z",
     "shell.execute_reply": "2025-05-28T07:18:56.969016Z",
     "shell.execute_reply.started": "2025-05-28T03:31:50.070155Z"
    },
    "papermill": {
     "duration": 16.814432,
     "end_time": "2025-05-28T07:18:56.969179",
     "exception": false,
     "start_time": "2025-05-28T07:18:40.154747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.12.5\n",
      "Uninstalling transformers-4.12.5:\n",
      "  Successfully uninstalled transformers-4.12.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: ../input/nbme-pip-wheels\n",
      "Processing /kaggle/input/nbme-pip-wheels/transformers-4.16.2-py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "allennlp 2.8.0 requires transformers<4.13,>=4.1, but you have transformers 4.16.2 which is incompatible.\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.10.3\n",
      "transformers.__version__: 4.16.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "os.system('pip uninstall -y transformers')\n",
    "os.system('python -m pip install --no-index --find-links=../input/nbme-pip-wheels transformers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813de7d",
   "metadata": {
    "papermill": {
     "duration": 0.023456,
     "end_time": "2025-05-28T07:18:57.012235",
     "exception": false,
     "start_time": "2025-05-28T07:18:56.988779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper functions for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83c74f70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:57.060903Z",
     "iopub.status.busy": "2025-05-28T07:18:57.057558Z",
     "iopub.status.idle": "2025-05-28T07:18:57.063663Z",
     "shell.execute_reply": "2025-05-28T07:18:57.062895Z",
     "shell.execute_reply.started": "2025-05-28T03:32:05.041244Z"
    },
    "papermill": {
     "duration": 0.032313,
     "end_time": "2025-05-28T07:18:57.063840",
     "exception": false,
     "start_time": "2025-05-28T07:18:57.031527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0df937",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:57.118596Z",
     "iopub.status.busy": "2025-05-28T07:18:57.117664Z",
     "iopub.status.idle": "2025-05-28T07:18:57.121114Z",
     "shell.execute_reply": "2025-05-28T07:18:57.120191Z",
     "shell.execute_reply.started": "2025-05-28T03:32:05.052406Z"
    },
    "papermill": {
     "duration": 0.036752,
     "end_time": "2025-05-28T07:18:57.121324",
     "exception": false,
     "start_time": "2025-05-28T07:18:57.084572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f53b0e",
   "metadata": {
    "papermill": {
     "duration": 0.02627,
     "end_time": "2025-05-28T07:18:57.179862",
     "exception": false,
     "start_time": "2025-05-28T07:18:57.153592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d76c746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:57.243867Z",
     "iopub.status.busy": "2025-05-28T07:18:57.241073Z",
     "iopub.status.idle": "2025-05-28T07:18:57.251404Z",
     "shell.execute_reply": "2025-05-28T07:18:57.250735Z",
     "shell.execute_reply.started": "2025-05-28T03:32:05.073209Z"
    },
    "papermill": {
     "duration": 0.042434,
     "end_time": "2025-05-28T07:18:57.251550",
     "exception": false,
     "start_time": "2025-05-28T07:18:57.209116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c065f7c",
   "metadata": {
    "papermill": {
     "duration": 0.021233,
     "end_time": "2025-05-28T07:18:57.294488",
     "exception": false,
     "start_time": "2025-05-28T07:18:57.273255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4813644d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:57.358762Z",
     "iopub.status.busy": "2025-05-28T07:18:57.357799Z",
     "iopub.status.idle": "2025-05-28T07:18:58.294596Z",
     "shell.execute_reply": "2025-05-28T07:18:58.293678Z",
     "shell.execute_reply.started": "2025-05-28T03:32:05.085676Z"
    },
    "papermill": {
     "duration": 0.968381,
     "end_time": "2025-05-28T07:18:58.294797",
     "exception": false,
     "start_time": "2025-05-28T07:18:57.326416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (14300, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[dad with recent heart attcak]</td>\n",
       "      <td>[696 724]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[mom with \"thyroid disease]</td>\n",
       "      <td>[668 693]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[chest pressure]</td>\n",
       "      <td>[203 217]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>[intermittent episodes, episode]</td>\n",
       "      <td>[70 91, 176 183]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[felt as if he were going to pass out]</td>\n",
       "      <td>[222 258]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num                              annotation          location\n",
       "0  00016_000         0      16            0          [dad with recent heart attcak]         [696 724]\n",
       "1  00016_001         0      16            1             [mom with \"thyroid disease]         [668 693]\n",
       "2  00016_002         0      16            2                        [chest pressure]         [203 217]\n",
       "3  00016_003         0      16            3        [intermittent episodes, episode]  [70 91, 176 183]\n",
       "4  00016_004         0      16            4  [felt as if he were going to pass out]         [222 258]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.shape: (143, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Chest-pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Lightheaded</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_num  case_num                                       feature_text\n",
       "0            0         0  Family-history-of-MI-OR-Family-history-of-myoc...\n",
       "1            1         0                 Family-history-of-thyroid-disorder\n",
       "2            2         0                                     Chest-pressure\n",
       "3            3         0                              Intermittent-symptoms\n",
       "4            4         0                                        Lightheaded"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_notes.shape: (42146, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pn_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17-year-old male, has come to the student heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17 yo male with recurrent palpitations for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Dillon Cleveland is a 17 y.o. male patient wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a 17 yo m c/o palpitation started 3 mos ago; \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17yo male with no pmh here for evaluation of p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pn_num  case_num                                         pn_history\n",
       "0       0         0  17-year-old male, has come to the student heal...\n",
       "1       1         0  17 yo male with recurrent palpitations for the...\n",
       "2       2         0  Dillon Cleveland is a 17 y.o. male patient wit...\n",
       "3       3         0  a 17 yo m c/o palpitation started 3 mos ago; \\...\n",
       "4       4         0  17yo male with no pmh here for evaluation of p..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval)\n",
    "train['location'] = train['location'].apply(ast.literal_eval)\n",
    "features = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\n",
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "features = preprocess_features(features)\n",
    "patient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n",
    "\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "display(train.head())\n",
    "print(f\"features.shape: {features.shape}\")\n",
    "display(features.head())\n",
    "print(f\"patient_notes.shape: {patient_notes.shape}\")\n",
    "display(patient_notes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dff8dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:58.357098Z",
     "iopub.status.busy": "2025-05-28T07:18:58.355884Z",
     "iopub.status.idle": "2025-05-28T07:18:58.398787Z",
     "shell.execute_reply": "2025-05-28T07:18:58.399365Z",
     "shell.execute_reply.started": "2025-05-28T03:32:05.956743Z"
    },
    "papermill": {
     "duration": 0.075438,
     "end_time": "2025-05-28T07:18:58.399573",
     "exception": false,
     "start_time": "2025-05-28T07:18:58.324135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[dad with recent heart attcak]</td>\n",
       "      <td>[696 724]</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[mom with \"thyroid disease]</td>\n",
       "      <td>[668 693]</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[chest pressure]</td>\n",
       "      <td>[203 217]</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>[intermittent episodes, episode]</td>\n",
       "      <td>[70 91, 176 183]</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[felt as if he were going to pass out]</td>\n",
       "      <td>[222 258]</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num                              annotation          location                                       feature_text                                         pn_history\n",
       "0  00016_000         0      16            0          [dad with recent heart attcak]         [696 724]  Family-history-of-MI-OR-Family-history-of-myoc...  HPI: 17yo M presents with palpitations. Patien...\n",
       "1  00016_001         0      16            1             [mom with \"thyroid disease]         [668 693]                 Family-history-of-thyroid-disorder  HPI: 17yo M presents with palpitations. Patien...\n",
       "2  00016_002         0      16            2                        [chest pressure]         [203 217]                                     Chest-pressure  HPI: 17yo M presents with palpitations. Patien...\n",
       "3  00016_003         0      16            3        [intermittent episodes, episode]  [70 91, 176 183]                              Intermittent-symptoms  HPI: 17yo M presents with palpitations. Patien...\n",
       "4  00016_004         0      16            4  [felt as if he were going to pass out]         [222 258]                                        Lightheaded  HPI: 17yo M presents with palpitations. Patien..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e310666d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:58.488001Z",
     "iopub.status.busy": "2025-05-28T07:18:58.469331Z",
     "iopub.status.idle": "2025-05-28T07:18:58.554212Z",
     "shell.execute_reply": "2025-05-28T07:18:58.554664Z",
     "shell.execute_reply.started": "2025-05-28T03:32:05.996630Z"
    },
    "papermill": {
     "duration": 0.125789,
     "end_time": "2025-05-28T07:18:58.554875",
     "exception": false,
     "start_time": "2025-05-28T07:18:58.429086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de3affa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:58.626974Z",
     "iopub.status.busy": "2025-05-28T07:18:58.624182Z",
     "iopub.status.idle": "2025-05-28T07:18:58.638889Z",
     "shell.execute_reply": "2025-05-28T07:18:58.638067Z",
     "shell.execute_reply.started": "2025-05-28T03:32:06.082636Z"
    },
    "papermill": {
     "duration": 0.057847,
     "end_time": "2025-05-28T07:18:58.639063",
     "exception": false,
     "start_time": "2025-05-28T07:18:58.581216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8185\n",
       "0    4399\n",
       "2    1292\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['annotation_length'] = train['annotation'].apply(len)\n",
    "display(train['annotation_length'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3231d246",
   "metadata": {
    "papermill": {
     "duration": 0.024885,
     "end_time": "2025-05-28T07:18:58.707958",
     "exception": false,
     "start_time": "2025-05-28T07:18:58.683073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a30ea61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:58.759927Z",
     "iopub.status.busy": "2025-05-28T07:18:58.758891Z",
     "iopub.status.idle": "2025-05-28T07:18:58.772773Z",
     "shell.execute_reply": "2025-05-28T07:18:58.772287Z",
     "shell.execute_reply.started": "2025-05-28T03:32:06.094566Z"
    },
    "papermill": {
     "duration": 0.042898,
     "end_time": "2025-05-28T07:18:58.772933",
     "exception": false,
     "start_time": "2025-05-28T07:18:58.730035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    2860\n",
       "1    2860\n",
       "2    2860\n",
       "3    2860\n",
       "4    2860\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e04c026b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:58.826445Z",
     "iopub.status.busy": "2025-05-28T07:18:58.825525Z",
     "iopub.status.idle": "2025-05-28T07:18:58.828127Z",
     "shell.execute_reply": "2025-05-28T07:18:58.827573Z",
     "shell.execute_reply.started": "2025-05-28T03:32:06.118135Z"
    },
    "papermill": {
     "duration": 0.032636,
     "end_time": "2025-05-28T07:18:58.828249",
     "exception": false,
     "start_time": "2025-05-28T07:18:58.795613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    display(train.groupby('fold').size())\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d81827",
   "metadata": {
    "papermill": {
     "duration": 0.026537,
     "end_time": "2025-05-28T07:18:58.878963",
     "exception": false,
     "start_time": "2025-05-28T07:18:58.852426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "185bc34f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:58.930363Z",
     "iopub.status.busy": "2025-05-28T07:18:58.929839Z",
     "iopub.status.idle": "2025-05-28T07:18:59.068276Z",
     "shell.execute_reply": "2025-05-28T07:18:59.068683Z",
     "shell.execute_reply.started": "2025-05-28T03:32:06.123549Z"
    },
    "papermill": {
     "duration": 0.166601,
     "end_time": "2025-05-28T07:18:59.068890",
     "exception": false,
     "start_time": "2025-05-28T07:18:58.902289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model, do_lower_case=True)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb1568",
   "metadata": {
    "papermill": {
     "duration": 0.022717,
     "end_time": "2025-05-28T07:18:59.115150",
     "exception": false,
     "start_time": "2025-05-28T07:18:59.092433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a4c2518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:18:59.177252Z",
     "iopub.status.busy": "2025-05-28T07:18:59.175436Z",
     "iopub.status.idle": "2025-05-28T07:19:26.386497Z",
     "shell.execute_reply": "2025-05-28T07:19:26.386009Z",
     "shell.execute_reply.started": "2025-05-28T03:32:06.268111Z"
    },
    "papermill": {
     "duration": 27.247684,
     "end_time": "2025-05-28T07:19:26.386615",
     "exception": false,
     "start_time": "2025-05-28T07:18:59.138931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73be62ac4f3a490c96cad587ca95e0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pn_history max(lengths): 329\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56eb1b7643ce40178c721ed7d58aa795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_text max(lengths): 29\n",
      "max_len: 361\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "for text_col in ['pn_history']:\n",
    "    pn_history_lengths = []\n",
    "    tk0 = tqdm(patient_notes[text_col].fillna(\"\").values, total=len(patient_notes))\n",
    "    for text in tk0:\n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "        pn_history_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(pn_history_lengths)}')\n",
    "\n",
    "for text_col in ['feature_text']:\n",
    "    features_lengths = []\n",
    "    tk0 = tqdm(features[text_col].fillna(\"\").values, total=len(features))\n",
    "    for text in tk0:\n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "        features_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(features_lengths)}')\n",
    "\n",
    "CFG.max_len = max(pn_history_lengths) + max(features_lengths) + 3 # cls & sep & sep\n",
    "CFG.max_len = min(CFG.max_len, 512)\n",
    "\n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28b4506b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:19:26.452729Z",
     "iopub.status.busy": "2025-05-28T07:19:26.451786Z",
     "iopub.status.idle": "2025-05-28T07:19:26.454414Z",
     "shell.execute_reply": "2025-05-28T07:19:26.455200Z",
     "shell.execute_reply.started": "2025-05-28T03:32:30.983333Z"
    },
    "papermill": {
     "duration": 0.041863,
     "end_time": "2025-05-28T07:19:26.455426",
     "exception": false,
     "start_time": "2025-05-28T07:19:26.413563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text, feature_text):\n",
    "    inputs = cfg.tokenizer(text, feature_text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=CFG.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def create_label(cfg, text, annotation_length, location_list):\n",
    "    encoded = cfg.tokenizer(text,\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=CFG.max_len,\n",
    "                            padding=\"max_length\",\n",
    "                            return_offsets_mapping=True)\n",
    "    offset_mapping = encoded['offset_mapping']\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1\n",
    "    if annotation_length != 0:\n",
    "        for location in location_list:\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                        start_idx = idx - 1\n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                        end_idx = idx + 1\n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label[start_idx:end_idx] = 1\n",
    "    return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "        self.annotation_lengths = df['annotation_length'].values\n",
    "        self.locations = df['location'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, \n",
    "                               self.pn_historys[item], \n",
    "                               self.feature_texts[item])\n",
    "        label = create_label(self.cfg, \n",
    "                             self.pn_historys[item], \n",
    "                             self.annotation_lengths[item], \n",
    "                             self.locations[item])\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f333ab",
   "metadata": {
    "papermill": {
     "duration": 0.023897,
     "end_time": "2025-05-28T07:19:26.507089",
     "exception": false,
     "start_time": "2025-05-28T07:19:26.483192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "606a9f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:19:26.565743Z",
     "iopub.status.busy": "2025-05-28T07:19:26.565113Z",
     "iopub.status.idle": "2025-05-28T07:19:26.567338Z",
     "shell.execute_reply": "2025-05-28T07:19:26.567734Z",
     "shell.execute_reply.started": "2025-05-28T03:32:30.996527Z"
    },
    "papermill": {
     "duration": 0.036687,
     "end_time": "2025-05-28T07:19:26.567913",
     "exception": false,
     "start_time": "2025-05-28T07:19:26.531226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913729a",
   "metadata": {
    "papermill": {
     "duration": 0.023956,
     "end_time": "2025-05-28T07:19:26.617055",
     "exception": false,
     "start_time": "2025-05-28T07:19:26.593099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helpler functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8903ec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:19:26.690061Z",
     "iopub.status.busy": "2025-05-28T07:19:26.680758Z",
     "iopub.status.idle": "2025-05-28T07:19:26.693089Z",
     "shell.execute_reply": "2025-05-28T07:19:26.693492Z",
     "shell.execute_reply.started": "2025-05-28T03:32:31.020299Z"
    },
    "papermill": {
     "duration": 0.052141,
     "end_time": "2025-05-28T07:19:26.693639",
     "exception": false,
     "start_time": "2025-05-28T07:19:26.641498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "becb0249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:19:26.764096Z",
     "iopub.status.busy": "2025-05-28T07:19:26.757358Z",
     "iopub.status.idle": "2025-05-28T07:19:26.766946Z",
     "shell.execute_reply": "2025-05-28T07:19:26.766436Z",
     "shell.execute_reply.started": "2025-05-28T03:32:31.045856Z"
    },
    "papermill": {
     "duration": 0.048067,
     "end_time": "2025-05-28T07:19:26.767083",
     "exception": false,
     "start_time": "2025-05-28T07:19:26.719016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_texts = valid_folds['pn_history'].values\n",
    "    valid_labels = create_labels_for_scoring(valid_folds)\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler=='linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler=='cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    best_score = 0.\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n",
    "        \n",
    "        # scoring\n",
    "        char_probs = get_char_probs(valid_texts, predictions, CFG.tokenizer)\n",
    "        results = get_results(char_probs, th=0.5)\n",
    "        preds = get_predictions(results)\n",
    "        score = get_score(valid_labels, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "            \n",
    "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    if predictions.shape[1] < CFG.max_len:\n",
    "        pad_width = CFG.max_len - predictions.shape[1]\n",
    "        predictions = np.pad(predictions, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "    elif predictions.shape[1] > CFG.max_len:\n",
    "        predictions = predictions[:, :CFG.max_len]  # truncate 超出的部份\n",
    "        \n",
    "    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "\n",
    "    logits_path = OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_logits.npy\"\n",
    "    np.save(logits_path, predictions)\n",
    "    LOGGER.info(f\"Saved logits to {logits_path}\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "203188e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:19:26.826844Z",
     "iopub.status.busy": "2025-05-28T07:19:26.825759Z",
     "iopub.status.idle": "2025-05-28T09:54:47.021900Z",
     "shell.execute_reply": "2025-05-28T09:54:47.021374Z",
     "shell.execute_reply.started": "2025-05-28T03:32:31.065347Z"
    },
    "papermill": {
     "duration": 9320.230164,
     "end_time": "2025-05-28T09:54:47.022044",
     "exception": false,
     "start_time": "2025-05-28T07:19:26.791880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Some weights of the model checkpoint at /kaggle/input/bio-clinicalbert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/2860] Elapsed 0m 1s (remain 83m 31s) Loss: 0.5139(0.5139) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][100/2860] Elapsed 0m 21s (remain 9m 55s) Loss: 0.0522(0.0997) Grad: 6329.4341  LR: 0.00001999  \n",
      "Epoch: [1][200/2860] Elapsed 0m 41s (remain 9m 13s) Loss: 0.0566(0.0770) Grad: 12654.4209  LR: 0.00001997  \n",
      "Epoch: [1][300/2860] Elapsed 1m 1s (remain 8m 46s) Loss: 0.0860(0.0677) Grad: 14439.2285  LR: 0.00001994  \n",
      "Epoch: [1][400/2860] Elapsed 1m 21s (remain 8m 22s) Loss: 0.0165(0.0607) Grad: 6220.5068  LR: 0.00001989  \n",
      "Epoch: [1][500/2860] Elapsed 1m 42s (remain 8m 0s) Loss: 0.0855(0.0553) Grad: 28005.2227  LR: 0.00001983  \n",
      "Epoch: [1][600/2860] Elapsed 2m 2s (remain 7m 39s) Loss: 0.0110(0.0514) Grad: 6843.9878  LR: 0.00001976  \n",
      "Epoch: [1][700/2860] Elapsed 2m 22s (remain 7m 17s) Loss: 0.0403(0.0486) Grad: 9159.4736  LR: 0.00001967  \n",
      "Epoch: [1][800/2860] Elapsed 2m 42s (remain 6m 57s) Loss: 0.0826(0.0465) Grad: 14833.6621  LR: 0.00001957  \n",
      "Epoch: [1][900/2860] Elapsed 3m 2s (remain 6m 36s) Loss: 0.0700(0.0447) Grad: 20145.8301  LR: 0.00001946  \n",
      "Epoch: [1][1000/2860] Elapsed 3m 22s (remain 6m 15s) Loss: 0.0470(0.0433) Grad: 10967.4678  LR: 0.00001934  \n",
      "Epoch: [1][1100/2860] Elapsed 3m 42s (remain 5m 55s) Loss: 0.0405(0.0423) Grad: 11834.9326  LR: 0.00001920  \n",
      "Epoch: [1][1200/2860] Elapsed 4m 2s (remain 5m 34s) Loss: 0.0027(0.0410) Grad: 1992.7042  LR: 0.00001905  \n",
      "Epoch: [1][1300/2860] Elapsed 4m 22s (remain 5m 14s) Loss: 0.0130(0.0397) Grad: 5279.6870  LR: 0.00001889  \n",
      "Epoch: [1][1400/2860] Elapsed 4m 42s (remain 4m 54s) Loss: 0.0835(0.0387) Grad: 27433.9824  LR: 0.00001871  \n",
      "Epoch: [1][1500/2860] Elapsed 5m 2s (remain 4m 34s) Loss: 0.0124(0.0376) Grad: 7393.2471  LR: 0.00001853  \n",
      "Epoch: [1][1600/2860] Elapsed 5m 22s (remain 4m 13s) Loss: 0.0002(0.0366) Grad: 142.5894  LR: 0.00001833  \n",
      "Epoch: [1][1700/2860] Elapsed 5m 42s (remain 3m 53s) Loss: 0.0079(0.0356) Grad: 5143.0391  LR: 0.00001812  \n",
      "Epoch: [1][1800/2860] Elapsed 6m 2s (remain 3m 33s) Loss: 0.0145(0.0349) Grad: 2840.0696  LR: 0.00001790  \n",
      "Epoch: [1][1900/2860] Elapsed 6m 22s (remain 3m 13s) Loss: 0.0265(0.0342) Grad: 20393.8164  LR: 0.00001767  \n",
      "Epoch: [1][2000/2860] Elapsed 6m 42s (remain 2m 52s) Loss: 0.0448(0.0337) Grad: 7912.0269  LR: 0.00001743  \n",
      "Epoch: [1][2100/2860] Elapsed 7m 2s (remain 2m 32s) Loss: 0.0137(0.0329) Grad: 9143.8828  LR: 0.00001718  \n",
      "Epoch: [1][2200/2860] Elapsed 7m 23s (remain 2m 12s) Loss: 0.0097(0.0323) Grad: 12317.5918  LR: 0.00001692  \n",
      "Epoch: [1][2300/2860] Elapsed 7m 43s (remain 1m 52s) Loss: 0.0096(0.0317) Grad: 7295.7476  LR: 0.00001666  \n",
      "Epoch: [1][2400/2860] Elapsed 8m 3s (remain 1m 32s) Loss: 0.0020(0.0312) Grad: 3977.9080  LR: 0.00001638  \n",
      "Epoch: [1][2500/2860] Elapsed 8m 23s (remain 1m 12s) Loss: 0.0085(0.0307) Grad: 17733.4766  LR: 0.00001609  \n",
      "Epoch: [1][2600/2860] Elapsed 8m 43s (remain 0m 52s) Loss: 0.0086(0.0302) Grad: 26213.9004  LR: 0.00001580  \n",
      "Epoch: [1][2700/2860] Elapsed 9m 3s (remain 0m 31s) Loss: 0.0042(0.0298) Grad: 13415.9062  LR: 0.00001550  \n",
      "Epoch: [1][2800/2860] Elapsed 9m 23s (remain 0m 11s) Loss: 0.0282(0.0293) Grad: 23960.6387  LR: 0.00001519  \n",
      "Epoch: [1][2859/2860] Elapsed 9m 35s (remain 0m 0s) Loss: 0.0012(0.0291) Grad: 2838.3823  LR: 0.00001500  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 3m 7s) Loss: 0.0172(0.0172) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 34s) Loss: 0.0113(0.0138) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0008(0.0174) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0006(0.0153) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.0197(0.0175) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0007(0.0176) \n",
      "EVAL: [600/715] Elapsed 0m 33s (remain 0m 6s) Loss: 0.0053(0.0176) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0015(0.0168) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0073(0.0168) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0291  avg_val_loss: 0.0168  time: 617s\n",
      "Epoch 1 - Score: 0.6732\n",
      "Epoch 1 - Save Best Score: 0.6732 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/2860] Elapsed 0m 0s (remain 18m 44s) Loss: 0.0006(0.0006) Grad: 2928.0110  LR: 0.00001500  \n",
      "Epoch: [2][100/2860] Elapsed 0m 20s (remain 9m 19s) Loss: 0.0005(0.0157) Grad: 2058.2883  LR: 0.00001468  \n",
      "Epoch: [2][200/2860] Elapsed 0m 40s (remain 8m 56s) Loss: 0.0134(0.0152) Grad: 18741.0723  LR: 0.00001435  \n",
      "Epoch: [2][300/2860] Elapsed 1m 0s (remain 8m 35s) Loss: 0.0229(0.0149) Grad: 60436.3477  LR: 0.00001402  \n",
      "Epoch: [2][400/2860] Elapsed 1m 20s (remain 8m 14s) Loss: 0.0043(0.0150) Grad: 19763.1855  LR: 0.00001368  \n",
      "Epoch: [2][500/2860] Elapsed 1m 40s (remain 7m 54s) Loss: 0.0032(0.0147) Grad: 13222.9238  LR: 0.00001334  \n",
      "Epoch: [2][600/2860] Elapsed 2m 0s (remain 7m 33s) Loss: 0.0007(0.0147) Grad: 5546.5913  LR: 0.00001299  \n",
      "Epoch: [2][700/2860] Elapsed 2m 20s (remain 7m 13s) Loss: 0.0001(0.0146) Grad: 892.2025  LR: 0.00001264  \n",
      "Epoch: [2][800/2860] Elapsed 2m 40s (remain 6m 53s) Loss: 0.0080(0.0145) Grad: 20212.1172  LR: 0.00001228  \n",
      "Epoch: [2][900/2860] Elapsed 3m 0s (remain 6m 33s) Loss: 0.0200(0.0148) Grad: 42949.0820  LR: 0.00001192  \n",
      "Epoch: [2][1000/2860] Elapsed 3m 21s (remain 6m 13s) Loss: 0.0399(0.0150) Grad: 78539.0156  LR: 0.00001156  \n",
      "Epoch: [2][1100/2860] Elapsed 3m 41s (remain 5m 53s) Loss: 0.0025(0.0147) Grad: 10266.7783  LR: 0.00001120  \n",
      "Epoch: [2][1200/2860] Elapsed 4m 1s (remain 5m 33s) Loss: 0.1251(0.0148) Grad: 85681.3125  LR: 0.00001084  \n",
      "Epoch: [2][1300/2860] Elapsed 4m 21s (remain 5m 12s) Loss: 0.0092(0.0148) Grad: 209773.2500  LR: 0.00001047  \n",
      "Epoch: [2][1400/2860] Elapsed 4m 41s (remain 4m 52s) Loss: 0.0052(0.0146) Grad: 13595.0889  LR: 0.00001011  \n",
      "Epoch: [2][1500/2860] Elapsed 5m 1s (remain 4m 32s) Loss: 0.0010(0.0147) Grad: 5988.9478  LR: 0.00000974  \n",
      "Epoch: [2][1600/2860] Elapsed 5m 21s (remain 4m 12s) Loss: 0.0000(0.0145) Grad: 162.4514  LR: 0.00000937  \n",
      "Epoch: [2][1700/2860] Elapsed 5m 41s (remain 3m 52s) Loss: 0.0503(0.0145) Grad: 36971.6602  LR: 0.00000901  \n",
      "Epoch: [2][1800/2860] Elapsed 6m 1s (remain 3m 32s) Loss: 0.0405(0.0146) Grad: 82507.2578  LR: 0.00000865  \n",
      "Epoch: [2][1900/2860] Elapsed 6m 21s (remain 3m 12s) Loss: 0.0208(0.0144) Grad: 62732.1562  LR: 0.00000828  \n",
      "Epoch: [2][2000/2860] Elapsed 6m 41s (remain 2m 52s) Loss: 0.0051(0.0143) Grad: 92901.0781  LR: 0.00000792  \n",
      "Epoch: [2][2100/2860] Elapsed 7m 1s (remain 2m 32s) Loss: 0.0034(0.0143) Grad: 31567.9590  LR: 0.00000757  \n",
      "Epoch: [2][2200/2860] Elapsed 7m 21s (remain 2m 12s) Loss: 0.0058(0.0142) Grad: 88437.9766  LR: 0.00000721  \n",
      "Epoch: [2][2300/2860] Elapsed 7m 41s (remain 1m 52s) Loss: 0.0018(0.0141) Grad: 20029.1895  LR: 0.00000686  \n",
      "Epoch: [2][2400/2860] Elapsed 8m 1s (remain 1m 32s) Loss: 0.0053(0.0141) Grad: 77021.2734  LR: 0.00000652  \n",
      "Epoch: [2][2500/2860] Elapsed 8m 21s (remain 1m 12s) Loss: 0.0000(0.0140) Grad: 591.5644  LR: 0.00000618  \n",
      "Epoch: [2][2600/2860] Elapsed 8m 41s (remain 0m 51s) Loss: 0.0438(0.0139) Grad: 162298.2656  LR: 0.00000584  \n",
      "Epoch: [2][2700/2860] Elapsed 9m 1s (remain 0m 31s) Loss: 0.0002(0.0139) Grad: 7165.2749  LR: 0.00000551  \n",
      "Epoch: [2][2800/2860] Elapsed 9m 21s (remain 0m 11s) Loss: 0.0115(0.0138) Grad: 173034.1562  LR: 0.00000519  \n",
      "Epoch: [2][2859/2860] Elapsed 9m 33s (remain 0m 0s) Loss: 0.0046(0.0138) Grad: 37672.5859  LR: 0.00000500  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 2m 48s) Loss: 0.0220(0.0220) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 34s) Loss: 0.0065(0.0160) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0010(0.0178) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0001(0.0154) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.0071(0.0178) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0001(0.0184) \n",
      "EVAL: [600/715] Elapsed 0m 33s (remain 0m 6s) Loss: 0.0012(0.0187) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0008(0.0179) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0121(0.0180) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0138  avg_val_loss: 0.0180  time: 616s\n",
      "Epoch 2 - Score: 0.7021\n",
      "Epoch 2 - Save Best Score: 0.7021 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/2860] Elapsed 0m 0s (remain 19m 17s) Loss: 0.0274(0.0274) Grad: 60961.4453  LR: 0.00000500  \n",
      "Epoch: [3][100/2860] Elapsed 0m 20s (remain 9m 18s) Loss: 0.0009(0.0130) Grad: 32806.3164  LR: 0.00000468  \n",
      "Epoch: [3][200/2860] Elapsed 0m 40s (remain 8m 55s) Loss: 0.0013(0.0131) Grad: 12246.9219  LR: 0.00000438  \n",
      "Epoch: [3][300/2860] Elapsed 1m 0s (remain 8m 34s) Loss: 0.0635(0.0127) Grad: 70046.7422  LR: 0.00000408  \n",
      "Epoch: [3][400/2860] Elapsed 1m 20s (remain 8m 14s) Loss: 0.0006(0.0121) Grad: 4878.9912  LR: 0.00000379  \n",
      "Epoch: [3][500/2860] Elapsed 1m 40s (remain 7m 54s) Loss: 0.0725(0.0120) Grad: 176722.9219  LR: 0.00000350  \n",
      "Epoch: [3][600/2860] Elapsed 2m 0s (remain 7m 33s) Loss: 0.0014(0.0125) Grad: 6821.2642  LR: 0.00000323  \n",
      "Epoch: [3][700/2860] Elapsed 2m 20s (remain 7m 13s) Loss: 0.0016(0.0123) Grad: 8625.8271  LR: 0.00000297  \n",
      "Epoch: [3][800/2860] Elapsed 2m 40s (remain 6m 53s) Loss: 0.0889(0.0123) Grad: 117942.0938  LR: 0.00000271  \n",
      "Epoch: [3][900/2860] Elapsed 3m 0s (remain 6m 33s) Loss: 0.0115(0.0121) Grad: 40660.0586  LR: 0.00000246  \n",
      "Epoch: [3][1000/2860] Elapsed 3m 21s (remain 6m 13s) Loss: 0.0039(0.0120) Grad: 20202.7012  LR: 0.00000223  \n",
      "Epoch: [3][1100/2860] Elapsed 3m 41s (remain 5m 53s) Loss: 0.0004(0.0120) Grad: 2885.7693  LR: 0.00000200  \n",
      "Epoch: [3][1200/2860] Elapsed 4m 1s (remain 5m 33s) Loss: 0.0354(0.0118) Grad: 51492.6328  LR: 0.00000179  \n",
      "Epoch: [3][1300/2860] Elapsed 4m 21s (remain 5m 13s) Loss: 0.0002(0.0116) Grad: 795.4290  LR: 0.00000159  \n",
      "Epoch: [3][1400/2860] Elapsed 4m 41s (remain 4m 52s) Loss: 0.0000(0.0117) Grad: 912.1229  LR: 0.00000139  \n",
      "Epoch: [3][1500/2860] Elapsed 5m 1s (remain 4m 32s) Loss: 0.0037(0.0118) Grad: 18850.2969  LR: 0.00000121  \n",
      "Epoch: [3][1600/2860] Elapsed 5m 21s (remain 4m 12s) Loss: 0.0004(0.0118) Grad: 6440.3428  LR: 0.00000104  \n",
      "Epoch: [3][1700/2860] Elapsed 5m 41s (remain 3m 52s) Loss: 0.0073(0.0117) Grad: 29433.5469  LR: 0.00000089  \n",
      "Epoch: [3][1800/2860] Elapsed 6m 1s (remain 3m 32s) Loss: 0.0110(0.0115) Grad: 39439.1602  LR: 0.00000074  \n",
      "Epoch: [3][1900/2860] Elapsed 6m 21s (remain 3m 12s) Loss: 0.0050(0.0116) Grad: 41729.3555  LR: 0.00000061  \n",
      "Epoch: [3][2000/2860] Elapsed 6m 41s (remain 2m 52s) Loss: 0.0001(0.0116) Grad: 1537.9639  LR: 0.00000049  \n",
      "Epoch: [3][2100/2860] Elapsed 7m 1s (remain 2m 32s) Loss: 0.0032(0.0116) Grad: 35043.3242  LR: 0.00000038  \n",
      "Epoch: [3][2200/2860] Elapsed 7m 21s (remain 2m 12s) Loss: 0.0059(0.0118) Grad: 41030.4219  LR: 0.00000029  \n",
      "Epoch: [3][2300/2860] Elapsed 7m 41s (remain 1m 52s) Loss: 0.0237(0.0117) Grad: 249021.0469  LR: 0.00000021  \n",
      "Epoch: [3][2400/2860] Elapsed 8m 1s (remain 1m 32s) Loss: 0.0003(0.0117) Grad: 6986.2476  LR: 0.00000014  \n",
      "Epoch: [3][2500/2860] Elapsed 8m 21s (remain 1m 12s) Loss: 0.0047(0.0116) Grad: 41404.5273  LR: 0.00000009  \n",
      "Epoch: [3][2600/2860] Elapsed 8m 41s (remain 0m 51s) Loss: 0.0000(0.0116) Grad: 441.3059  LR: 0.00000004  \n",
      "Epoch: [3][2700/2860] Elapsed 9m 1s (remain 0m 31s) Loss: 0.0046(0.0114) Grad: 103092.1484  LR: 0.00000002  \n",
      "Epoch: [3][2800/2860] Elapsed 9m 22s (remain 0m 11s) Loss: 0.0011(0.0115) Grad: 24487.6543  LR: 0.00000000  \n",
      "Epoch: [3][2859/2860] Elapsed 9m 33s (remain 0m 0s) Loss: 0.0395(0.0116) Grad: 128131.7891  LR: 0.00000000  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 2m 53s) Loss: 0.0168(0.0168) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 34s) Loss: 0.0057(0.0165) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0023(0.0180) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0000(0.0156) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.0063(0.0179) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0001(0.0186) \n",
      "EVAL: [600/715] Elapsed 0m 33s (remain 0m 6s) Loss: 0.0003(0.0187) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0002(0.0180) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0130(0.0181) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0116  avg_val_loss: 0.0181  time: 616s\n",
      "Epoch 3 - Score: 0.7094\n",
      "Epoch 3 - Save Best Score: 0.7094 Model\n",
      "Saved logits to ./-kaggle-input-bio-clinicalbert_fold0_logits.npy\n",
      "========== fold: 0 result ==========\n",
      "Score: 0.7094\n",
      "========== fold: 1 training ==========\n",
      "Some weights of the model checkpoint at /kaggle/input/bio-clinicalbert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/2860] Elapsed 0m 0s (remain 18m 58s) Loss: 0.8348(0.8348) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][100/2860] Elapsed 0m 20s (remain 9m 18s) Loss: 0.0735(0.1310) Grad: 5724.9097  LR: 0.00001999  \n",
      "Epoch: [1][200/2860] Elapsed 0m 40s (remain 8m 56s) Loss: 0.0311(0.0910) Grad: 5009.6509  LR: 0.00001997  \n",
      "Epoch: [1][300/2860] Elapsed 1m 0s (remain 8m 35s) Loss: 0.0154(0.0752) Grad: 3528.9739  LR: 0.00001994  \n",
      "Epoch: [1][400/2860] Elapsed 1m 20s (remain 8m 14s) Loss: 0.0333(0.0652) Grad: 8273.8535  LR: 0.00001989  \n",
      "Epoch: [1][500/2860] Elapsed 1m 40s (remain 7m 54s) Loss: 0.0774(0.0584) Grad: 11149.1143  LR: 0.00001983  \n",
      "Epoch: [1][600/2860] Elapsed 2m 0s (remain 7m 34s) Loss: 0.0143(0.0538) Grad: 2740.6284  LR: 0.00001976  \n",
      "Epoch: [1][700/2860] Elapsed 2m 20s (remain 7m 13s) Loss: 0.1090(0.0510) Grad: 8836.4414  LR: 0.00001967  \n",
      "Epoch: [1][800/2860] Elapsed 2m 40s (remain 6m 53s) Loss: 0.0037(0.0479) Grad: 2654.8113  LR: 0.00001957  \n",
      "Epoch: [1][900/2860] Elapsed 3m 1s (remain 6m 33s) Loss: 0.0092(0.0457) Grad: 2569.9536  LR: 0.00001946  \n",
      "Epoch: [1][1000/2860] Elapsed 3m 21s (remain 6m 13s) Loss: 0.0737(0.0438) Grad: 15097.7627  LR: 0.00001934  \n",
      "Epoch: [1][1100/2860] Elapsed 3m 41s (remain 5m 53s) Loss: 0.0153(0.0421) Grad: 5822.5962  LR: 0.00001920  \n",
      "Epoch: [1][1200/2860] Elapsed 4m 1s (remain 5m 33s) Loss: 0.0025(0.0405) Grad: 1154.5179  LR: 0.00001905  \n",
      "Epoch: [1][1300/2860] Elapsed 4m 21s (remain 5m 13s) Loss: 0.0013(0.0389) Grad: 361.7845  LR: 0.00001889  \n",
      "Epoch: [1][1400/2860] Elapsed 4m 41s (remain 4m 52s) Loss: 0.0426(0.0379) Grad: 9884.7334  LR: 0.00001871  \n",
      "Epoch: [1][1500/2860] Elapsed 5m 1s (remain 4m 32s) Loss: 0.0004(0.0366) Grad: 685.0038  LR: 0.00001853  \n",
      "Epoch: [1][1600/2860] Elapsed 5m 21s (remain 4m 12s) Loss: 0.0129(0.0356) Grad: 4137.0068  LR: 0.00001833  \n",
      "Epoch: [1][1700/2860] Elapsed 5m 41s (remain 3m 52s) Loss: 0.0138(0.0346) Grad: 8781.3350  LR: 0.00001812  \n",
      "Epoch: [1][1800/2860] Elapsed 6m 1s (remain 3m 32s) Loss: 0.0154(0.0339) Grad: 5115.4839  LR: 0.00001790  \n",
      "Epoch: [1][1900/2860] Elapsed 6m 21s (remain 3m 12s) Loss: 0.0207(0.0330) Grad: 7361.4849  LR: 0.00001767  \n",
      "Epoch: [1][2000/2860] Elapsed 6m 41s (remain 2m 52s) Loss: 0.0410(0.0323) Grad: 6413.6284  LR: 0.00001743  \n",
      "Epoch: [1][2100/2860] Elapsed 7m 1s (remain 2m 32s) Loss: 0.0001(0.0317) Grad: 427.8522  LR: 0.00001718  \n",
      "Epoch: [1][2200/2860] Elapsed 7m 21s (remain 2m 12s) Loss: 0.0006(0.0312) Grad: 765.5817  LR: 0.00001692  \n",
      "Epoch: [1][2300/2860] Elapsed 7m 41s (remain 1m 52s) Loss: 0.0219(0.0307) Grad: 19272.4629  LR: 0.00001666  \n",
      "Epoch: [1][2400/2860] Elapsed 8m 1s (remain 1m 32s) Loss: 0.0167(0.0302) Grad: 8871.2695  LR: 0.00001638  \n",
      "Epoch: [1][2500/2860] Elapsed 8m 21s (remain 1m 12s) Loss: 0.0318(0.0297) Grad: 9858.0352  LR: 0.00001609  \n",
      "Epoch: [1][2600/2860] Elapsed 8m 42s (remain 0m 51s) Loss: 0.0193(0.0294) Grad: 11053.3096  LR: 0.00001580  \n",
      "Epoch: [1][2700/2860] Elapsed 9m 2s (remain 0m 31s) Loss: 0.0071(0.0289) Grad: 2658.4290  LR: 0.00001550  \n",
      "Epoch: [1][2800/2860] Elapsed 9m 22s (remain 0m 11s) Loss: 0.0233(0.0284) Grad: 9991.6709  LR: 0.00001519  \n",
      "Epoch: [1][2859/2860] Elapsed 9m 34s (remain 0m 0s) Loss: 0.0423(0.0283) Grad: 14842.5107  LR: 0.00001500  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 3m 5s) Loss: 0.0067(0.0067) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 34s) Loss: 0.0004(0.0136) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0001(0.0144) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0069(0.0128) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.0804(0.0149) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0141(0.0158) \n",
      "EVAL: [600/715] Elapsed 0m 33s (remain 0m 6s) Loss: 0.0071(0.0164) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0002(0.0154) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0051(0.0154) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0283  avg_val_loss: 0.0154  time: 616s\n",
      "Epoch 1 - Score: 0.6983\n",
      "Epoch 1 - Save Best Score: 0.6983 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/2860] Elapsed 0m 0s (remain 19m 2s) Loss: 0.0004(0.0004) Grad: 4656.2319  LR: 0.00001500  \n",
      "Epoch: [2][100/2860] Elapsed 0m 20s (remain 9m 19s) Loss: 0.0042(0.0090) Grad: 12933.2666  LR: 0.00001468  \n",
      "Epoch: [2][200/2860] Elapsed 0m 40s (remain 8m 56s) Loss: 0.0252(0.0116) Grad: 60536.2461  LR: 0.00001435  \n",
      "Epoch: [2][300/2860] Elapsed 1m 0s (remain 8m 35s) Loss: 0.0002(0.0122) Grad: 2099.5459  LR: 0.00001402  \n",
      "Epoch: [2][400/2860] Elapsed 1m 20s (remain 8m 14s) Loss: 0.0294(0.0118) Grad: 58285.8867  LR: 0.00001368  \n",
      "Epoch: [2][500/2860] Elapsed 1m 40s (remain 7m 54s) Loss: 0.0075(0.0118) Grad: 40625.2617  LR: 0.00001334  \n",
      "Epoch: [2][600/2860] Elapsed 2m 0s (remain 7m 34s) Loss: 0.0385(0.0122) Grad: 44228.8242  LR: 0.00001299  \n",
      "Epoch: [2][700/2860] Elapsed 2m 20s (remain 7m 14s) Loss: 0.0015(0.0117) Grad: 8918.5352  LR: 0.00001264  \n",
      "Epoch: [2][800/2860] Elapsed 2m 41s (remain 6m 53s) Loss: 0.0001(0.0120) Grad: 450.0037  LR: 0.00001228  \n",
      "Epoch: [2][900/2860] Elapsed 3m 1s (remain 6m 33s) Loss: 0.0074(0.0118) Grad: 53642.0938  LR: 0.00001192  \n",
      "Epoch: [2][1000/2860] Elapsed 3m 21s (remain 6m 13s) Loss: 0.0154(0.0118) Grad: 32614.8613  LR: 0.00001156  \n",
      "Epoch: [2][1100/2860] Elapsed 3m 41s (remain 5m 53s) Loss: 0.0069(0.0118) Grad: 24755.6641  LR: 0.00001120  \n",
      "Epoch: [2][1200/2860] Elapsed 4m 1s (remain 5m 33s) Loss: 0.0011(0.0120) Grad: 7703.1528  LR: 0.00001084  \n",
      "Epoch: [2][1300/2860] Elapsed 4m 21s (remain 5m 13s) Loss: 0.0017(0.0122) Grad: 9085.8818  LR: 0.00001047  \n",
      "Epoch: [2][1400/2860] Elapsed 4m 41s (remain 4m 53s) Loss: 0.0139(0.0121) Grad: 80483.9531  LR: 0.00001011  \n",
      "Epoch: [2][1500/2860] Elapsed 5m 1s (remain 4m 32s) Loss: 0.0371(0.0120) Grad: 48701.8281  LR: 0.00000974  \n",
      "Epoch: [2][1600/2860] Elapsed 5m 21s (remain 4m 12s) Loss: 0.0004(0.0118) Grad: 2964.8765  LR: 0.00000937  \n",
      "Epoch: [2][1700/2860] Elapsed 5m 41s (remain 3m 52s) Loss: 0.0015(0.0119) Grad: 22769.2168  LR: 0.00000901  \n",
      "Epoch: [2][1800/2860] Elapsed 6m 1s (remain 3m 32s) Loss: 0.0110(0.0119) Grad: 30466.1504  LR: 0.00000865  \n",
      "Epoch: [2][1900/2860] Elapsed 6m 21s (remain 3m 12s) Loss: 0.0035(0.0121) Grad: 29895.9727  LR: 0.00000828  \n",
      "Epoch: [2][2000/2860] Elapsed 6m 41s (remain 2m 52s) Loss: 0.0044(0.0122) Grad: 28413.2383  LR: 0.00000792  \n",
      "Epoch: [2][2100/2860] Elapsed 7m 1s (remain 2m 32s) Loss: 0.0465(0.0123) Grad: 132476.5469  LR: 0.00000757  \n",
      "Epoch: [2][2200/2860] Elapsed 7m 21s (remain 2m 12s) Loss: 0.0098(0.0122) Grad: 99858.3359  LR: 0.00000721  \n",
      "Epoch: [2][2300/2860] Elapsed 7m 41s (remain 1m 52s) Loss: 0.0483(0.0123) Grad: 272954.4062  LR: 0.00000686  \n",
      "Epoch: [2][2400/2860] Elapsed 8m 2s (remain 1m 32s) Loss: 0.0179(0.0124) Grad: 51168.6562  LR: 0.00000652  \n",
      "Epoch: [2][2500/2860] Elapsed 8m 22s (remain 1m 12s) Loss: 0.0036(0.0124) Grad: 22533.6348  LR: 0.00000618  \n",
      "Epoch: [2][2600/2860] Elapsed 8m 42s (remain 0m 51s) Loss: 0.0222(0.0125) Grad: 430135.6250  LR: 0.00000584  \n",
      "Epoch: [2][2700/2860] Elapsed 9m 2s (remain 0m 31s) Loss: 0.0001(0.0125) Grad: 996.3544  LR: 0.00000551  \n",
      "Epoch: [2][2800/2860] Elapsed 9m 22s (remain 0m 11s) Loss: 0.0001(0.0126) Grad: 620.5687  LR: 0.00000519  \n",
      "Epoch: [2][2859/2860] Elapsed 9m 34s (remain 0m 0s) Loss: 0.0396(0.0126) Grad: 55917.9375  LR: 0.00000500  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 2m 59s) Loss: 0.0038(0.0038) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 34s) Loss: 0.0001(0.0155) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0000(0.0163) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0004(0.0145) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.0891(0.0170) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0249(0.0177) \n",
      "EVAL: [600/715] Elapsed 0m 33s (remain 0m 6s) Loss: 0.0012(0.0184) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0000(0.0172) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0011(0.0172) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0126  avg_val_loss: 0.0172  time: 616s\n",
      "Epoch 2 - Score: 0.7097\n",
      "Epoch 2 - Save Best Score: 0.7097 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/2860] Elapsed 0m 0s (remain 20m 6s) Loss: 0.0004(0.0004) Grad: 6404.8047  LR: 0.00000500  \n",
      "Epoch: [3][100/2860] Elapsed 0m 20s (remain 9m 19s) Loss: 0.0001(0.0103) Grad: 682.3864  LR: 0.00000468  \n",
      "Epoch: [3][200/2860] Elapsed 0m 40s (remain 8m 56s) Loss: 0.0000(0.0117) Grad: 204.1152  LR: 0.00000438  \n",
      "Epoch: [3][300/2860] Elapsed 1m 0s (remain 8m 35s) Loss: 0.0000(0.0123) Grad: 157.0724  LR: 0.00000408  \n",
      "Epoch: [3][400/2860] Elapsed 1m 20s (remain 8m 14s) Loss: 0.0626(0.0118) Grad: 241943.3281  LR: 0.00000379  \n",
      "Epoch: [3][500/2860] Elapsed 1m 40s (remain 7m 54s) Loss: 0.1045(0.0120) Grad: 162157.4688  LR: 0.00000350  \n",
      "Epoch: [3][600/2860] Elapsed 2m 0s (remain 7m 34s) Loss: 0.0074(0.0118) Grad: 31718.0664  LR: 0.00000323  \n",
      "Epoch: [3][700/2860] Elapsed 2m 20s (remain 7m 13s) Loss: 0.0015(0.0117) Grad: 11807.1533  LR: 0.00000297  \n",
      "Epoch: [3][800/2860] Elapsed 2m 40s (remain 6m 53s) Loss: 0.0243(0.0115) Grad: 30712.6172  LR: 0.00000271  \n",
      "Epoch: [3][900/2860] Elapsed 3m 0s (remain 6m 33s) Loss: 0.0137(0.0113) Grad: 50483.9648  LR: 0.00000246  \n",
      "Epoch: [3][1000/2860] Elapsed 3m 21s (remain 6m 13s) Loss: 0.0043(0.0110) Grad: 28537.7148  LR: 0.00000223  \n",
      "Epoch: [3][1100/2860] Elapsed 3m 41s (remain 5m 53s) Loss: 0.0071(0.0113) Grad: 32356.3633  LR: 0.00000200  \n",
      "Epoch: [3][1200/2860] Elapsed 4m 1s (remain 5m 33s) Loss: 0.0001(0.0110) Grad: 686.7237  LR: 0.00000179  \n",
      "Epoch: [3][1300/2860] Elapsed 4m 21s (remain 5m 13s) Loss: 0.0231(0.0111) Grad: 37744.3711  LR: 0.00000159  \n",
      "Epoch: [3][1400/2860] Elapsed 4m 41s (remain 4m 52s) Loss: 0.0116(0.0111) Grad: 45376.3867  LR: 0.00000139  \n",
      "Epoch: [3][1500/2860] Elapsed 5m 1s (remain 4m 32s) Loss: 0.0262(0.0110) Grad: 76563.3359  LR: 0.00000121  \n",
      "Epoch: [3][1600/2860] Elapsed 5m 21s (remain 4m 12s) Loss: 0.0053(0.0110) Grad: 9657.8047  LR: 0.00000104  \n",
      "Epoch: [3][1700/2860] Elapsed 5m 41s (remain 3m 52s) Loss: 0.0111(0.0108) Grad: 34651.7695  LR: 0.00000089  \n",
      "Epoch: [3][1800/2860] Elapsed 6m 1s (remain 3m 32s) Loss: 0.0079(0.0110) Grad: 32071.5586  LR: 0.00000074  \n",
      "Epoch: [3][1900/2860] Elapsed 6m 21s (remain 3m 12s) Loss: 0.0098(0.0110) Grad: 39431.5977  LR: 0.00000061  \n",
      "Epoch: [3][2000/2860] Elapsed 6m 41s (remain 2m 52s) Loss: 0.0004(0.0109) Grad: 6723.9795  LR: 0.00000049  \n",
      "Epoch: [3][2100/2860] Elapsed 7m 1s (remain 2m 32s) Loss: 0.0000(0.0110) Grad: 135.8594  LR: 0.00000038  \n",
      "Epoch: [3][2200/2860] Elapsed 7m 21s (remain 2m 12s) Loss: 0.0010(0.0111) Grad: 31259.7734  LR: 0.00000029  \n",
      "Epoch: [3][2300/2860] Elapsed 7m 41s (remain 1m 52s) Loss: 0.0259(0.0111) Grad: 149052.9219  LR: 0.00000021  \n",
      "Epoch: [3][2400/2860] Elapsed 8m 1s (remain 1m 32s) Loss: 0.0124(0.0111) Grad: 79418.1953  LR: 0.00000014  \n",
      "Epoch: [3][2500/2860] Elapsed 8m 21s (remain 1m 12s) Loss: 0.0067(0.0111) Grad: 135739.7031  LR: 0.00000009  \n",
      "Epoch: [3][2600/2860] Elapsed 8m 42s (remain 0m 51s) Loss: 0.0037(0.0111) Grad: 38493.8945  LR: 0.00000004  \n",
      "Epoch: [3][2700/2860] Elapsed 9m 2s (remain 0m 31s) Loss: 0.0086(0.0111) Grad: 108968.7266  LR: 0.00000002  \n",
      "Epoch: [3][2800/2860] Elapsed 9m 22s (remain 0m 11s) Loss: 0.0026(0.0112) Grad: 24743.6387  LR: 0.00000000  \n",
      "Epoch: [3][2859/2860] Elapsed 9m 34s (remain 0m 0s) Loss: 0.0104(0.0111) Grad: 59795.3281  LR: 0.00000000  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 3m 12s) Loss: 0.0033(0.0033) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 34s) Loss: 0.0001(0.0158) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0000(0.0170) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0002(0.0149) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.0951(0.0175) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0270(0.0185) \n",
      "EVAL: [600/715] Elapsed 0m 33s (remain 0m 6s) Loss: 0.0036(0.0191) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0000(0.0180) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0009(0.0181) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0111  avg_val_loss: 0.0181  time: 617s\n",
      "Epoch 3 - Score: 0.7056\n",
      "Saved logits to ./-kaggle-input-bio-clinicalbert_fold1_logits.npy\n",
      "========== fold: 1 result ==========\n",
      "Score: 0.7097\n",
      "========== fold: 2 training ==========\n",
      "Some weights of the model checkpoint at /kaggle/input/bio-clinicalbert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/2860] Elapsed 0m 0s (remain 20m 8s) Loss: 0.6302(0.6302) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][100/2860] Elapsed 0m 20s (remain 9m 19s) Loss: 0.0698(0.1174) Grad: 3418.3516  LR: 0.00001999  \n",
      "Epoch: [1][200/2860] Elapsed 0m 40s (remain 8m 57s) Loss: 0.0371(0.0864) Grad: 6147.9146  LR: 0.00001997  \n",
      "Epoch: [1][300/2860] Elapsed 1m 0s (remain 8m 36s) Loss: 0.0088(0.0722) Grad: 3161.9783  LR: 0.00001994  \n",
      "Epoch: [1][400/2860] Elapsed 1m 20s (remain 8m 16s) Loss: 0.0410(0.0646) Grad: 6338.6567  LR: 0.00001989  \n",
      "Epoch: [1][500/2860] Elapsed 1m 41s (remain 7m 56s) Loss: 0.0202(0.0588) Grad: 6483.4087  LR: 0.00001983  \n",
      "Epoch: [1][600/2860] Elapsed 2m 1s (remain 7m 35s) Loss: 0.0146(0.0545) Grad: 5702.0444  LR: 0.00001976  \n",
      "Epoch: [1][700/2860] Elapsed 2m 21s (remain 7m 15s) Loss: 0.0182(0.0510) Grad: 4113.4282  LR: 0.00001967  \n",
      "Epoch: [1][800/2860] Elapsed 2m 41s (remain 6m 55s) Loss: 0.0132(0.0487) Grad: 4918.2070  LR: 0.00001957  \n",
      "Epoch: [1][900/2860] Elapsed 3m 1s (remain 6m 35s) Loss: 0.0052(0.0469) Grad: 1477.0912  LR: 0.00001946  \n",
      "Epoch: [1][1000/2860] Elapsed 3m 21s (remain 6m 14s) Loss: 0.0046(0.0447) Grad: 2303.8962  LR: 0.00001934  \n",
      "Epoch: [1][1100/2860] Elapsed 3m 41s (remain 5m 54s) Loss: 0.0035(0.0434) Grad: 2076.8953  LR: 0.00001920  \n",
      "Epoch: [1][1200/2860] Elapsed 4m 1s (remain 5m 34s) Loss: 0.0028(0.0416) Grad: 1218.4266  LR: 0.00001905  \n",
      "Epoch: [1][1300/2860] Elapsed 4m 22s (remain 5m 14s) Loss: 0.0292(0.0404) Grad: 6360.7998  LR: 0.00001889  \n",
      "Epoch: [1][1400/2860] Elapsed 4m 42s (remain 4m 53s) Loss: 0.0050(0.0393) Grad: 2134.0776  LR: 0.00001871  \n",
      "Epoch: [1][1500/2860] Elapsed 5m 2s (remain 4m 33s) Loss: 0.0120(0.0382) Grad: 5003.0151  LR: 0.00001853  \n",
      "Epoch: [1][1600/2860] Elapsed 5m 22s (remain 4m 13s) Loss: 0.0031(0.0372) Grad: 1918.2776  LR: 0.00001833  \n",
      "Epoch: [1][1700/2860] Elapsed 5m 42s (remain 3m 53s) Loss: 0.0003(0.0365) Grad: 169.6534  LR: 0.00001812  \n",
      "Epoch: [1][1800/2860] Elapsed 6m 2s (remain 3m 33s) Loss: 0.0043(0.0357) Grad: 1420.9546  LR: 0.00001790  \n",
      "Epoch: [1][1900/2860] Elapsed 6m 22s (remain 3m 12s) Loss: 0.0013(0.0349) Grad: 787.9317  LR: 0.00001767  \n",
      "Epoch: [1][2000/2860] Elapsed 6m 42s (remain 2m 52s) Loss: 0.0108(0.0342) Grad: 6906.7583  LR: 0.00001743  \n",
      "Epoch: [1][2100/2860] Elapsed 7m 2s (remain 2m 32s) Loss: 0.0121(0.0335) Grad: 7112.6440  LR: 0.00001718  \n",
      "Epoch: [1][2200/2860] Elapsed 7m 22s (remain 2m 12s) Loss: 0.0216(0.0328) Grad: 11939.6025  LR: 0.00001692  \n",
      "Epoch: [1][2300/2860] Elapsed 7m 42s (remain 1m 52s) Loss: 0.0169(0.0323) Grad: 9091.7588  LR: 0.00001666  \n",
      "Epoch: [1][2400/2860] Elapsed 8m 2s (remain 1m 32s) Loss: 0.0121(0.0317) Grad: 10424.9834  LR: 0.00001638  \n",
      "Epoch: [1][2500/2860] Elapsed 8m 22s (remain 1m 12s) Loss: 0.0373(0.0312) Grad: 14165.0918  LR: 0.00001609  \n",
      "Epoch: [1][2600/2860] Elapsed 8m 43s (remain 0m 52s) Loss: 0.0451(0.0308) Grad: 14873.1670  LR: 0.00001580  \n",
      "Epoch: [1][2700/2860] Elapsed 9m 3s (remain 0m 31s) Loss: 0.0657(0.0304) Grad: 16535.5098  LR: 0.00001550  \n",
      "Epoch: [1][2800/2860] Elapsed 9m 23s (remain 0m 11s) Loss: 0.0006(0.0300) Grad: 427.0540  LR: 0.00001519  \n",
      "Epoch: [1][2859/2860] Elapsed 9m 35s (remain 0m 0s) Loss: 0.0090(0.0297) Grad: 4390.7612  LR: 0.00001500  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 3m 16s) Loss: 0.0120(0.0120) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 34s) Loss: 0.0011(0.0142) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0002(0.0177) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0141(0.0154) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.0443(0.0172) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0438(0.0172) \n",
      "EVAL: [600/715] Elapsed 0m 33s (remain 0m 6s) Loss: 0.0021(0.0176) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0033(0.0166) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0007(0.0164) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0297  avg_val_loss: 0.0164  time: 617s\n",
      "Epoch 1 - Score: 0.6854\n",
      "Epoch 1 - Save Best Score: 0.6854 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/2860] Elapsed 0m 0s (remain 20m 15s) Loss: 0.0004(0.0004) Grad: 3107.0720  LR: 0.00001500  \n",
      "Epoch: [2][100/2860] Elapsed 0m 20s (remain 9m 20s) Loss: 0.0030(0.0141) Grad: 13094.3418  LR: 0.00001468  \n",
      "Epoch: [2][200/2860] Elapsed 0m 40s (remain 8m 56s) Loss: 0.0328(0.0130) Grad: 46634.5859  LR: 0.00001435  \n",
      "Epoch: [2][300/2860] Elapsed 1m 0s (remain 8m 35s) Loss: 0.0131(0.0143) Grad: 38412.4648  LR: 0.00001402  \n",
      "Epoch: [2][400/2860] Elapsed 1m 20s (remain 8m 15s) Loss: 0.0236(0.0141) Grad: 39346.9180  LR: 0.00001368  \n",
      "Epoch: [2][500/2860] Elapsed 1m 40s (remain 7m 54s) Loss: 0.0007(0.0139) Grad: 4543.3628  LR: 0.00001334  \n",
      "Epoch: [2][600/2860] Elapsed 2m 0s (remain 7m 34s) Loss: 0.0187(0.0140) Grad: 30822.7129  LR: 0.00001299  \n",
      "Epoch: [2][700/2860] Elapsed 2m 21s (remain 7m 14s) Loss: 0.0383(0.0143) Grad: 75921.0938  LR: 0.00001264  \n",
      "Epoch: [2][800/2860] Elapsed 2m 41s (remain 6m 54s) Loss: 0.0079(0.0138) Grad: 30582.3281  LR: 0.00001228  \n",
      "Epoch: [2][900/2860] Elapsed 3m 1s (remain 6m 33s) Loss: 0.0005(0.0137) Grad: 7333.5327  LR: 0.00001192  \n",
      "Epoch: [2][1000/2860] Elapsed 3m 21s (remain 6m 13s) Loss: 0.0116(0.0136) Grad: 32653.2988  LR: 0.00001156  \n",
      "Epoch: [2][1100/2860] Elapsed 3m 41s (remain 5m 53s) Loss: 0.0031(0.0135) Grad: 18412.8555  LR: 0.00001120  \n",
      "Epoch: [2][1200/2860] Elapsed 4m 1s (remain 5m 33s) Loss: 0.0057(0.0141) Grad: 17273.9668  LR: 0.00001084  \n",
      "Epoch: [2][1300/2860] Elapsed 4m 21s (remain 5m 13s) Loss: 0.0008(0.0140) Grad: 4027.9507  LR: 0.00001047  \n",
      "Epoch: [2][1400/2860] Elapsed 4m 41s (remain 4m 53s) Loss: 0.0868(0.0139) Grad: 293358.3125  LR: 0.00001011  \n",
      "Epoch: [2][1500/2860] Elapsed 5m 1s (remain 4m 33s) Loss: 0.0000(0.0139) Grad: 233.1868  LR: 0.00000974  \n",
      "Epoch: [2][1600/2860] Elapsed 5m 21s (remain 4m 12s) Loss: 0.0010(0.0138) Grad: 4314.6128  LR: 0.00000937  \n",
      "Epoch: [2][1700/2860] Elapsed 5m 41s (remain 3m 52s) Loss: 0.0001(0.0136) Grad: 590.0073  LR: 0.00000901  \n",
      "Epoch: [2][1800/2860] Elapsed 6m 1s (remain 3m 32s) Loss: 0.0002(0.0137) Grad: 1099.5536  LR: 0.00000865  \n",
      "Epoch: [2][1900/2860] Elapsed 6m 21s (remain 3m 12s) Loss: 0.0006(0.0138) Grad: 2592.9771  LR: 0.00000828  \n",
      "Epoch: [2][2000/2860] Elapsed 6m 41s (remain 2m 52s) Loss: 0.0052(0.0140) Grad: 21423.0488  LR: 0.00000792  \n",
      "Epoch: [2][2100/2860] Elapsed 7m 1s (remain 2m 32s) Loss: 0.0214(0.0139) Grad: 115642.2109  LR: 0.00000757  \n",
      "Epoch: [2][2200/2860] Elapsed 7m 22s (remain 2m 12s) Loss: 0.0229(0.0137) Grad: 306542.3125  LR: 0.00000721  \n",
      "Epoch: [2][2300/2860] Elapsed 7m 42s (remain 1m 52s) Loss: 0.0001(0.0137) Grad: 559.1609  LR: 0.00000686  \n",
      "Epoch: [2][2400/2860] Elapsed 8m 2s (remain 1m 32s) Loss: 0.0082(0.0136) Grad: 157996.1250  LR: 0.00000652  \n",
      "Epoch: [2][2500/2860] Elapsed 8m 22s (remain 1m 12s) Loss: 0.0017(0.0136) Grad: 27862.4414  LR: 0.00000618  \n",
      "Epoch: [2][2600/2860] Elapsed 8m 42s (remain 0m 52s) Loss: 0.0161(0.0135) Grad: 74605.6172  LR: 0.00000584  \n",
      "Epoch: [2][2700/2860] Elapsed 9m 2s (remain 0m 31s) Loss: 0.0008(0.0137) Grad: 12864.0508  LR: 0.00000551  \n",
      "Epoch: [2][2800/2860] Elapsed 9m 22s (remain 0m 11s) Loss: 0.0003(0.0137) Grad: 2826.4443  LR: 0.00000519  \n",
      "Epoch: [2][2859/2860] Elapsed 9m 34s (remain 0m 0s) Loss: 0.0135(0.0137) Grad: 64197.5234  LR: 0.00000500  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 3m 20s) Loss: 0.0056(0.0056) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 35s) Loss: 0.0011(0.0163) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0000(0.0200) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0124(0.0174) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.0540(0.0196) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0336(0.0192) \n",
      "EVAL: [600/715] Elapsed 0m 33s (remain 0m 6s) Loss: 0.0007(0.0195) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0025(0.0182) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0002(0.0179) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0137  avg_val_loss: 0.0179  time: 617s\n",
      "Epoch 2 - Score: 0.6991\n",
      "Epoch 2 - Save Best Score: 0.6991 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/2860] Elapsed 0m 0s (remain 20m 41s) Loss: 0.0048(0.0048) Grad: 10630.3760  LR: 0.00000500  \n",
      "Epoch: [3][100/2860] Elapsed 0m 20s (remain 9m 20s) Loss: 0.0036(0.0113) Grad: 12803.0293  LR: 0.00000468  \n",
      "Epoch: [3][200/2860] Elapsed 0m 40s (remain 8m 56s) Loss: 0.0138(0.0110) Grad: 46988.2656  LR: 0.00000438  \n",
      "Epoch: [3][300/2860] Elapsed 1m 0s (remain 8m 35s) Loss: 0.0229(0.0108) Grad: 26436.8594  LR: 0.00000408  \n",
      "Epoch: [3][400/2860] Elapsed 1m 20s (remain 8m 14s) Loss: 0.0981(0.0115) Grad: 50130.1836  LR: 0.00000379  \n",
      "Epoch: [3][500/2860] Elapsed 1m 40s (remain 7m 54s) Loss: 0.0024(0.0108) Grad: 13224.0693  LR: 0.00000350  \n",
      "Epoch: [3][600/2860] Elapsed 2m 0s (remain 7m 33s) Loss: 0.0051(0.0109) Grad: 22823.7871  LR: 0.00000323  \n",
      "Epoch: [3][700/2860] Elapsed 2m 20s (remain 7m 13s) Loss: 0.0085(0.0114) Grad: 52948.3789  LR: 0.00000297  \n",
      "Epoch: [3][800/2860] Elapsed 2m 40s (remain 6m 53s) Loss: 0.0355(0.0116) Grad: 122103.2266  LR: 0.00000271  \n",
      "Epoch: [3][900/2860] Elapsed 3m 0s (remain 6m 33s) Loss: 0.0192(0.0118) Grad: 39895.2891  LR: 0.00000246  \n",
      "Epoch: [3][1000/2860] Elapsed 3m 21s (remain 6m 13s) Loss: 0.0024(0.0119) Grad: 12846.6279  LR: 0.00000223  \n",
      "Epoch: [3][1100/2860] Elapsed 3m 41s (remain 5m 53s) Loss: 0.0260(0.0121) Grad: 92062.0234  LR: 0.00000200  \n",
      "Epoch: [3][1200/2860] Elapsed 4m 1s (remain 5m 33s) Loss: 0.0182(0.0121) Grad: 36303.3398  LR: 0.00000179  \n",
      "Epoch: [3][1300/2860] Elapsed 4m 21s (remain 5m 12s) Loss: 0.0049(0.0120) Grad: 24288.3398  LR: 0.00000159  \n",
      "Epoch: [3][1400/2860] Elapsed 4m 41s (remain 4m 52s) Loss: 0.0017(0.0121) Grad: 15214.3291  LR: 0.00000139  \n",
      "Epoch: [3][1500/2860] Elapsed 5m 1s (remain 4m 32s) Loss: 0.0281(0.0123) Grad: 79887.9766  LR: 0.00000121  \n",
      "Epoch: [3][1600/2860] Elapsed 5m 21s (remain 4m 12s) Loss: 0.0014(0.0123) Grad: 6591.3462  LR: 0.00000104  \n",
      "Epoch: [3][1700/2860] Elapsed 5m 41s (remain 3m 52s) Loss: 0.0046(0.0123) Grad: 16092.5703  LR: 0.00000089  \n",
      "Epoch: [3][1800/2860] Elapsed 6m 1s (remain 3m 32s) Loss: 0.0000(0.0122) Grad: 85.7560  LR: 0.00000074  \n",
      "Epoch: [3][1900/2860] Elapsed 6m 21s (remain 3m 12s) Loss: 0.0083(0.0123) Grad: 39400.7383  LR: 0.00000061  \n",
      "Epoch: [3][2000/2860] Elapsed 6m 41s (remain 2m 52s) Loss: 0.0041(0.0122) Grad: 116300.1328  LR: 0.00000049  \n",
      "Epoch: [3][2100/2860] Elapsed 7m 1s (remain 2m 32s) Loss: 0.0033(0.0121) Grad: 24360.4609  LR: 0.00000038  \n",
      "Epoch: [3][2200/2860] Elapsed 7m 21s (remain 2m 12s) Loss: 0.0001(0.0121) Grad: 2381.1079  LR: 0.00000029  \n",
      "Epoch: [3][2300/2860] Elapsed 7m 41s (remain 1m 52s) Loss: 0.0039(0.0120) Grad: 44400.4922  LR: 0.00000021  \n",
      "Epoch: [3][2400/2860] Elapsed 8m 1s (remain 1m 32s) Loss: 0.0125(0.0120) Grad: 53623.1055  LR: 0.00000014  \n",
      "Epoch: [3][2500/2860] Elapsed 8m 21s (remain 1m 12s) Loss: 0.0033(0.0120) Grad: 49764.9492  LR: 0.00000009  \n",
      "Epoch: [3][2600/2860] Elapsed 8m 42s (remain 0m 51s) Loss: 0.0221(0.0120) Grad: 52054.8242  LR: 0.00000004  \n",
      "Epoch: [3][2700/2860] Elapsed 9m 2s (remain 0m 31s) Loss: 0.0097(0.0120) Grad: 32720.4102  LR: 0.00000002  \n",
      "Epoch: [3][2800/2860] Elapsed 9m 22s (remain 0m 11s) Loss: 0.0007(0.0119) Grad: 7865.0117  LR: 0.00000000  \n",
      "Epoch: [3][2859/2860] Elapsed 9m 33s (remain 0m 0s) Loss: 0.0757(0.0119) Grad: 114713.8828  LR: 0.00000000  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 3m 10s) Loss: 0.0067(0.0067) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 34s) Loss: 0.0017(0.0164) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0000(0.0198) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0128(0.0173) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.0513(0.0194) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0299(0.0190) \n",
      "EVAL: [600/715] Elapsed 0m 33s (remain 0m 6s) Loss: 0.0004(0.0194) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0034(0.0181) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0001(0.0178) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0119  avg_val_loss: 0.0178  time: 616s\n",
      "Epoch 3 - Score: 0.7022\n",
      "Epoch 3 - Save Best Score: 0.7022 Model\n",
      "Saved logits to ./-kaggle-input-bio-clinicalbert_fold2_logits.npy\n",
      "========== fold: 2 result ==========\n",
      "Score: 0.7022\n",
      "========== fold: 3 training ==========\n",
      "Some weights of the model checkpoint at /kaggle/input/bio-clinicalbert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/2860] Elapsed 0m 0s (remain 19m 58s) Loss: 0.6524(0.6524) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][100/2860] Elapsed 0m 20s (remain 9m 18s) Loss: 0.0095(0.1120) Grad: 1230.9208  LR: 0.00001999  \n",
      "Epoch: [1][200/2860] Elapsed 0m 40s (remain 8m 56s) Loss: 0.0311(0.0809) Grad: 4597.3247  LR: 0.00001997  \n",
      "Epoch: [1][300/2860] Elapsed 1m 0s (remain 8m 35s) Loss: 0.0189(0.0682) Grad: 2894.0806  LR: 0.00001994  \n",
      "Epoch: [1][400/2860] Elapsed 1m 20s (remain 8m 14s) Loss: 0.0197(0.0612) Grad: 3419.1472  LR: 0.00001989  \n",
      "Epoch: [1][500/2860] Elapsed 1m 40s (remain 7m 54s) Loss: 0.0794(0.0555) Grad: 15185.9932  LR: 0.00001983  \n",
      "Epoch: [1][600/2860] Elapsed 2m 0s (remain 7m 34s) Loss: 0.0857(0.0518) Grad: 8040.2427  LR: 0.00001976  \n",
      "Epoch: [1][700/2860] Elapsed 2m 20s (remain 7m 14s) Loss: 0.0454(0.0488) Grad: 6694.1626  LR: 0.00001967  \n",
      "Epoch: [1][800/2860] Elapsed 2m 41s (remain 6m 53s) Loss: 0.0539(0.0464) Grad: 13517.3535  LR: 0.00001957  \n",
      "Epoch: [1][900/2860] Elapsed 3m 1s (remain 6m 33s) Loss: 0.0006(0.0447) Grad: 265.2877  LR: 0.00001946  \n",
      "Epoch: [1][1000/2860] Elapsed 3m 21s (remain 6m 13s) Loss: 0.0170(0.0432) Grad: 4556.8330  LR: 0.00001934  \n",
      "Epoch: [1][1100/2860] Elapsed 3m 41s (remain 5m 53s) Loss: 0.0223(0.0417) Grad: 8249.3447  LR: 0.00001920  \n",
      "Epoch: [1][1200/2860] Elapsed 4m 1s (remain 5m 33s) Loss: 0.0146(0.0405) Grad: 4645.8843  LR: 0.00001905  \n",
      "Epoch: [1][1300/2860] Elapsed 4m 21s (remain 5m 13s) Loss: 0.0410(0.0393) Grad: 4128.3374  LR: 0.00001889  \n",
      "Epoch: [1][1400/2860] Elapsed 4m 41s (remain 4m 53s) Loss: 0.0668(0.0380) Grad: 8016.1709  LR: 0.00001871  \n",
      "Epoch: [1][1500/2860] Elapsed 5m 1s (remain 4m 32s) Loss: 0.0977(0.0370) Grad: 15585.0293  LR: 0.00001853  \n",
      "Epoch: [1][1600/2860] Elapsed 5m 21s (remain 4m 12s) Loss: 0.0067(0.0360) Grad: 2766.8782  LR: 0.00001833  \n",
      "Epoch: [1][1700/2860] Elapsed 5m 41s (remain 3m 52s) Loss: 0.0523(0.0352) Grad: 10231.8477  LR: 0.00001812  \n",
      "Epoch: [1][1800/2860] Elapsed 6m 1s (remain 3m 32s) Loss: 0.0081(0.0342) Grad: 2513.6826  LR: 0.00001790  \n",
      "Epoch: [1][1900/2860] Elapsed 6m 21s (remain 3m 12s) Loss: 0.0263(0.0335) Grad: 5769.6196  LR: 0.00001767  \n",
      "Epoch: [1][2000/2860] Elapsed 6m 41s (remain 2m 52s) Loss: 0.0152(0.0326) Grad: 5267.1646  LR: 0.00001743  \n",
      "Epoch: [1][2100/2860] Elapsed 7m 1s (remain 2m 32s) Loss: 0.0148(0.0321) Grad: 13290.1230  LR: 0.00001718  \n",
      "Epoch: [1][2200/2860] Elapsed 7m 21s (remain 2m 12s) Loss: 0.1389(0.0318) Grad: 30300.9043  LR: 0.00001692  \n",
      "Epoch: [1][2300/2860] Elapsed 7m 41s (remain 1m 52s) Loss: 0.0292(0.0313) Grad: 10281.9141  LR: 0.00001666  \n",
      "Epoch: [1][2400/2860] Elapsed 8m 2s (remain 1m 32s) Loss: 0.0246(0.0306) Grad: 13619.8633  LR: 0.00001638  \n",
      "Epoch: [1][2500/2860] Elapsed 8m 22s (remain 1m 12s) Loss: 0.0019(0.0301) Grad: 1780.0758  LR: 0.00001609  \n",
      "Epoch: [1][2600/2860] Elapsed 8m 42s (remain 0m 51s) Loss: 0.0314(0.0298) Grad: 19741.8613  LR: 0.00001580  \n",
      "Epoch: [1][2700/2860] Elapsed 9m 2s (remain 0m 31s) Loss: 0.0074(0.0293) Grad: 8349.9512  LR: 0.00001550  \n",
      "Epoch: [1][2800/2860] Elapsed 9m 22s (remain 0m 11s) Loss: 0.0040(0.0288) Grad: 3357.5811  LR: 0.00001519  \n",
      "Epoch: [1][2859/2860] Elapsed 9m 34s (remain 0m 0s) Loss: 0.0013(0.0285) Grad: 1700.3322  LR: 0.00001500  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 3m 13s) Loss: 0.0148(0.0148) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 34s) Loss: 0.0088(0.0134) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0001(0.0171) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0216(0.0164) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.0784(0.0166) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0576(0.0171) \n",
      "EVAL: [600/715] Elapsed 0m 33s (remain 0m 6s) Loss: 0.0056(0.0179) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0014(0.0165) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0005(0.0163) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0285  avg_val_loss: 0.0163  time: 616s\n",
      "Epoch 1 - Score: 0.6931\n",
      "Epoch 1 - Save Best Score: 0.6931 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/2860] Elapsed 0m 0s (remain 20m 19s) Loss: 0.0091(0.0091) Grad: 15328.6660  LR: 0.00001500  \n",
      "Epoch: [2][100/2860] Elapsed 0m 20s (remain 9m 20s) Loss: 0.0365(0.0126) Grad: 54642.0703  LR: 0.00001468  \n",
      "Epoch: [2][200/2860] Elapsed 0m 40s (remain 8m 56s) Loss: 0.0057(0.0128) Grad: 33473.5781  LR: 0.00001435  \n",
      "Epoch: [2][300/2860] Elapsed 1m 0s (remain 8m 35s) Loss: 0.0572(0.0129) Grad: 166866.9219  LR: 0.00001402  \n",
      "Epoch: [2][400/2860] Elapsed 1m 20s (remain 8m 14s) Loss: 0.0077(0.0129) Grad: 24969.0078  LR: 0.00001368  \n",
      "Epoch: [2][500/2860] Elapsed 1m 40s (remain 7m 54s) Loss: 0.0238(0.0133) Grad: 67654.0000  LR: 0.00001334  \n",
      "Epoch: [2][600/2860] Elapsed 2m 0s (remain 7m 34s) Loss: 0.0001(0.0127) Grad: 1318.3878  LR: 0.00001299  \n",
      "Epoch: [2][700/2860] Elapsed 2m 20s (remain 7m 13s) Loss: 0.0004(0.0132) Grad: 4849.6729  LR: 0.00001264  \n",
      "Epoch: [2][800/2860] Elapsed 2m 40s (remain 6m 53s) Loss: 0.0093(0.0130) Grad: 13959.7734  LR: 0.00001228  \n",
      "Epoch: [2][900/2860] Elapsed 3m 1s (remain 6m 33s) Loss: 0.0508(0.0130) Grad: 48036.8047  LR: 0.00001192  \n",
      "Epoch: [2][1000/2860] Elapsed 3m 21s (remain 6m 13s) Loss: 0.0254(0.0131) Grad: 75343.8594  LR: 0.00001156  \n",
      "Epoch: [2][1100/2860] Elapsed 3m 41s (remain 5m 53s) Loss: 0.0119(0.0131) Grad: 30767.8828  LR: 0.00001120  \n",
      "Epoch: [2][1200/2860] Elapsed 4m 1s (remain 5m 33s) Loss: 0.0000(0.0131) Grad: 922.8867  LR: 0.00001084  \n",
      "Epoch: [2][1300/2860] Elapsed 4m 21s (remain 5m 13s) Loss: 0.0119(0.0128) Grad: 23604.6309  LR: 0.00001047  \n",
      "Epoch: [2][1400/2860] Elapsed 4m 41s (remain 4m 53s) Loss: 0.0092(0.0128) Grad: 37611.2344  LR: 0.00001011  \n",
      "Epoch: [2][1500/2860] Elapsed 5m 1s (remain 4m 32s) Loss: 0.0157(0.0130) Grad: 168110.0000  LR: 0.00000974  \n",
      "Epoch: [2][1600/2860] Elapsed 5m 21s (remain 4m 12s) Loss: 0.0448(0.0132) Grad: 113638.9453  LR: 0.00000937  \n",
      "Epoch: [2][1700/2860] Elapsed 5m 41s (remain 3m 52s) Loss: 0.0123(0.0131) Grad: 16600.0762  LR: 0.00000901  \n",
      "Epoch: [2][1800/2860] Elapsed 6m 1s (remain 3m 32s) Loss: 0.0082(0.0131) Grad: 65648.1484  LR: 0.00000865  \n",
      "Epoch: [2][1900/2860] Elapsed 6m 21s (remain 3m 12s) Loss: 0.0003(0.0131) Grad: 1025.6479  LR: 0.00000828  \n",
      "Epoch: [2][2000/2860] Elapsed 6m 41s (remain 2m 52s) Loss: 0.0010(0.0130) Grad: 53445.6914  LR: 0.00000792  \n",
      "Epoch: [2][2100/2860] Elapsed 7m 1s (remain 2m 32s) Loss: 0.0001(0.0130) Grad: 880.0413  LR: 0.00000757  \n",
      "Epoch: [2][2200/2860] Elapsed 7m 21s (remain 2m 12s) Loss: 0.0003(0.0131) Grad: 3129.0989  LR: 0.00000721  \n",
      "Epoch: [2][2300/2860] Elapsed 7m 41s (remain 1m 52s) Loss: 0.0009(0.0130) Grad: 12237.4219  LR: 0.00000686  \n",
      "Epoch: [2][2400/2860] Elapsed 8m 1s (remain 1m 32s) Loss: 0.0132(0.0130) Grad: 45845.3789  LR: 0.00000652  \n",
      "Epoch: [2][2500/2860] Elapsed 8m 22s (remain 1m 12s) Loss: 0.0003(0.0130) Grad: 3786.2273  LR: 0.00000618  \n",
      "Epoch: [2][2600/2860] Elapsed 8m 42s (remain 0m 51s) Loss: 0.0302(0.0130) Grad: 162451.6562  LR: 0.00000584  \n",
      "Epoch: [2][2700/2860] Elapsed 9m 2s (remain 0m 31s) Loss: 0.0026(0.0130) Grad: 30368.0234  LR: 0.00000551  \n",
      "Epoch: [2][2800/2860] Elapsed 9m 22s (remain 0m 11s) Loss: 0.0554(0.0131) Grad: 224785.1094  LR: 0.00000519  \n",
      "Epoch: [2][2859/2860] Elapsed 9m 34s (remain 0m 0s) Loss: 0.0069(0.0131) Grad: 30038.6191  LR: 0.00000500  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 3m 35s) Loss: 0.0135(0.0135) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 35s) Loss: 0.0088(0.0135) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0001(0.0183) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 23s) Loss: 0.0237(0.0173) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.1093(0.0173) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0686(0.0182) \n",
      "EVAL: [600/715] Elapsed 0m 33s (remain 0m 6s) Loss: 0.0058(0.0189) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0004(0.0175) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0001(0.0173) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0131  avg_val_loss: 0.0173  time: 617s\n",
      "Epoch 2 - Score: 0.7053\n",
      "Epoch 2 - Save Best Score: 0.7053 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/2860] Elapsed 0m 0s (remain 20m 39s) Loss: 0.0040(0.0040) Grad: 11086.0439  LR: 0.00000500  \n",
      "Epoch: [3][100/2860] Elapsed 0m 20s (remain 9m 22s) Loss: 0.0152(0.0112) Grad: 64162.7578  LR: 0.00000468  \n",
      "Epoch: [3][200/2860] Elapsed 0m 40s (remain 8m 58s) Loss: 0.0220(0.0108) Grad: 95501.3047  LR: 0.00000438  \n",
      "Epoch: [3][300/2860] Elapsed 1m 0s (remain 8m 36s) Loss: 0.0101(0.0113) Grad: 23118.1543  LR: 0.00000408  \n",
      "Epoch: [3][400/2860] Elapsed 1m 20s (remain 8m 16s) Loss: 0.0000(0.0120) Grad: 160.9085  LR: 0.00000379  \n",
      "Epoch: [3][500/2860] Elapsed 1m 41s (remain 7m 55s) Loss: 0.0036(0.0113) Grad: 8198.0459  LR: 0.00000350  \n",
      "Epoch: [3][600/2860] Elapsed 2m 1s (remain 7m 35s) Loss: 0.0151(0.0115) Grad: 13852.1084  LR: 0.00000323  \n",
      "Epoch: [3][700/2860] Elapsed 2m 21s (remain 7m 14s) Loss: 0.0019(0.0114) Grad: 13883.2363  LR: 0.00000297  \n",
      "Epoch: [3][800/2860] Elapsed 2m 41s (remain 6m 54s) Loss: 0.0299(0.0118) Grad: 86762.7031  LR: 0.00000271  \n",
      "Epoch: [3][900/2860] Elapsed 3m 1s (remain 6m 33s) Loss: 0.0011(0.0118) Grad: 7546.4497  LR: 0.00000246  \n",
      "Epoch: [3][1000/2860] Elapsed 3m 21s (remain 6m 13s) Loss: 0.0059(0.0118) Grad: 25478.0527  LR: 0.00000223  \n",
      "Epoch: [3][1100/2860] Elapsed 3m 41s (remain 5m 53s) Loss: 0.0244(0.0117) Grad: 88849.5859  LR: 0.00000200  \n",
      "Epoch: [3][1200/2860] Elapsed 4m 1s (remain 5m 33s) Loss: 0.0009(0.0115) Grad: 7024.7100  LR: 0.00000179  \n",
      "Epoch: [3][1300/2860] Elapsed 4m 21s (remain 5m 13s) Loss: 0.0220(0.0113) Grad: 14678.2246  LR: 0.00000159  \n",
      "Epoch: [3][1400/2860] Elapsed 4m 41s (remain 4m 53s) Loss: 0.0037(0.0114) Grad: 44290.3359  LR: 0.00000139  \n",
      "Epoch: [3][1500/2860] Elapsed 5m 1s (remain 4m 32s) Loss: 0.0050(0.0112) Grad: 20118.4297  LR: 0.00000121  \n",
      "Epoch: [3][1600/2860] Elapsed 5m 21s (remain 4m 12s) Loss: 0.0244(0.0112) Grad: 37722.4609  LR: 0.00000104  \n",
      "Epoch: [3][1700/2860] Elapsed 5m 41s (remain 3m 52s) Loss: 0.0190(0.0114) Grad: 73136.4844  LR: 0.00000089  \n",
      "Epoch: [3][1800/2860] Elapsed 6m 1s (remain 3m 32s) Loss: 0.0149(0.0115) Grad: 86631.9766  LR: 0.00000074  \n",
      "Epoch: [3][1900/2860] Elapsed 6m 21s (remain 3m 12s) Loss: 0.0042(0.0115) Grad: 12464.4111  LR: 0.00000061  \n",
      "Epoch: [3][2000/2860] Elapsed 6m 41s (remain 2m 52s) Loss: 0.0153(0.0115) Grad: 141596.7812  LR: 0.00000049  \n",
      "Epoch: [3][2100/2860] Elapsed 7m 1s (remain 2m 32s) Loss: 0.0067(0.0116) Grad: 35425.5703  LR: 0.00000038  \n",
      "Epoch: [3][2200/2860] Elapsed 7m 21s (remain 2m 12s) Loss: 0.0266(0.0116) Grad: 272419.9062  LR: 0.00000029  \n",
      "Epoch: [3][2300/2860] Elapsed 7m 41s (remain 1m 52s) Loss: 0.0001(0.0115) Grad: 3878.8132  LR: 0.00000021  \n",
      "Epoch: [3][2400/2860] Elapsed 8m 1s (remain 1m 32s) Loss: 0.0105(0.0117) Grad: 77929.5234  LR: 0.00000014  \n",
      "Epoch: [3][2500/2860] Elapsed 8m 21s (remain 1m 11s) Loss: 0.0063(0.0116) Grad: 41048.3320  LR: 0.00000009  \n",
      "Epoch: [3][2600/2860] Elapsed 8m 41s (remain 0m 51s) Loss: 0.0003(0.0117) Grad: 4334.0532  LR: 0.00000004  \n",
      "Epoch: [3][2700/2860] Elapsed 9m 1s (remain 0m 31s) Loss: 0.0079(0.0116) Grad: 36861.3828  LR: 0.00000002  \n",
      "Epoch: [3][2800/2860] Elapsed 9m 21s (remain 0m 11s) Loss: 0.0049(0.0116) Grad: 85073.4609  LR: 0.00000000  \n",
      "Epoch: [3][2859/2860] Elapsed 9m 33s (remain 0m 0s) Loss: 0.0002(0.0116) Grad: 4446.5371  LR: 0.00000000  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 3m 5s) Loss: 0.0116(0.0116) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 34s) Loss: 0.0095(0.0140) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0000(0.0185) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0266(0.0177) \n",
      "EVAL: [400/715] Elapsed 0m 21s (remain 0m 17s) Loss: 0.1105(0.0177) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0721(0.0186) \n",
      "EVAL: [600/715] Elapsed 0m 32s (remain 0m 6s) Loss: 0.0049(0.0194) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0002(0.0180) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0000(0.0177) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0116  avg_val_loss: 0.0177  time: 615s\n",
      "Epoch 3 - Score: 0.7058\n",
      "Epoch 3 - Save Best Score: 0.7058 Model\n",
      "Saved logits to ./-kaggle-input-bio-clinicalbert_fold3_logits.npy\n",
      "========== fold: 3 result ==========\n",
      "Score: 0.7058\n",
      "========== fold: 4 training ==========\n",
      "Some weights of the model checkpoint at /kaggle/input/bio-clinicalbert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/2860] Elapsed 0m 0s (remain 19m 13s) Loss: 0.6882(0.6882) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][100/2860] Elapsed 0m 20s (remain 9m 15s) Loss: 0.0110(0.1106) Grad: 1747.7765  LR: 0.00001999  \n",
      "Epoch: [1][200/2860] Elapsed 0m 40s (remain 8m 53s) Loss: 0.0267(0.0812) Grad: 3644.8796  LR: 0.00001997  \n",
      "Epoch: [1][300/2860] Elapsed 1m 0s (remain 8m 32s) Loss: 0.0709(0.0671) Grad: 11914.4121  LR: 0.00001994  \n",
      "Epoch: [1][400/2860] Elapsed 1m 20s (remain 8m 12s) Loss: 0.0333(0.0599) Grad: 4507.2192  LR: 0.00001989  \n",
      "Epoch: [1][500/2860] Elapsed 1m 40s (remain 7m 52s) Loss: 0.0048(0.0549) Grad: 1611.9427  LR: 0.00001983  \n",
      "Epoch: [1][600/2860] Elapsed 2m 0s (remain 7m 32s) Loss: 0.0078(0.0510) Grad: 4699.8857  LR: 0.00001976  \n",
      "Epoch: [1][700/2860] Elapsed 2m 20s (remain 7m 12s) Loss: 0.0131(0.0475) Grad: 5813.2148  LR: 0.00001967  \n",
      "Epoch: [1][800/2860] Elapsed 2m 40s (remain 6m 51s) Loss: 0.0050(0.0455) Grad: 1958.6094  LR: 0.00001957  \n",
      "Epoch: [1][900/2860] Elapsed 3m 0s (remain 6m 32s) Loss: 0.0318(0.0429) Grad: 6454.1094  LR: 0.00001946  \n",
      "Epoch: [1][1000/2860] Elapsed 3m 20s (remain 6m 12s) Loss: 0.0213(0.0412) Grad: 5381.1021  LR: 0.00001934  \n",
      "Epoch: [1][1100/2860] Elapsed 3m 40s (remain 5m 52s) Loss: 0.0048(0.0396) Grad: 1812.9236  LR: 0.00001920  \n",
      "Epoch: [1][1200/2860] Elapsed 4m 0s (remain 5m 31s) Loss: 0.0107(0.0380) Grad: 3121.4622  LR: 0.00001905  \n",
      "Epoch: [1][1300/2860] Elapsed 4m 20s (remain 5m 11s) Loss: 0.0167(0.0367) Grad: 2749.1152  LR: 0.00001889  \n",
      "Epoch: [1][1400/2860] Elapsed 4m 40s (remain 4m 51s) Loss: 0.0048(0.0358) Grad: 1167.4500  LR: 0.00001871  \n",
      "Epoch: [1][1500/2860] Elapsed 5m 0s (remain 4m 31s) Loss: 0.0094(0.0349) Grad: 8199.4492  LR: 0.00001853  \n",
      "Epoch: [1][1600/2860] Elapsed 5m 20s (remain 4m 11s) Loss: 0.0004(0.0341) Grad: 130.7429  LR: 0.00001833  \n",
      "Epoch: [1][1700/2860] Elapsed 5m 40s (remain 3m 51s) Loss: 0.0252(0.0333) Grad: 6806.7246  LR: 0.00001812  \n",
      "Epoch: [1][1800/2860] Elapsed 6m 0s (remain 3m 31s) Loss: 0.0036(0.0327) Grad: 1104.9026  LR: 0.00001790  \n",
      "Epoch: [1][1900/2860] Elapsed 6m 20s (remain 3m 11s) Loss: 0.0139(0.0320) Grad: 3843.0964  LR: 0.00001767  \n",
      "Epoch: [1][2000/2860] Elapsed 6m 40s (remain 2m 51s) Loss: 0.0072(0.0313) Grad: 1914.3416  LR: 0.00001743  \n",
      "Epoch: [1][2100/2860] Elapsed 6m 59s (remain 2m 31s) Loss: 0.0143(0.0306) Grad: 8150.0156  LR: 0.00001718  \n",
      "Epoch: [1][2200/2860] Elapsed 7m 19s (remain 2m 11s) Loss: 0.0002(0.0302) Grad: 150.5429  LR: 0.00001692  \n",
      "Epoch: [1][2300/2860] Elapsed 7m 39s (remain 1m 51s) Loss: 0.0049(0.0296) Grad: 7327.2471  LR: 0.00001666  \n",
      "Epoch: [1][2400/2860] Elapsed 7m 59s (remain 1m 31s) Loss: 0.0079(0.0290) Grad: 3027.0815  LR: 0.00001638  \n",
      "Epoch: [1][2500/2860] Elapsed 8m 19s (remain 1m 11s) Loss: 0.0090(0.0286) Grad: 6523.0786  LR: 0.00001609  \n",
      "Epoch: [1][2600/2860] Elapsed 8m 39s (remain 0m 51s) Loss: 0.0499(0.0280) Grad: 20759.0488  LR: 0.00001580  \n",
      "Epoch: [1][2700/2860] Elapsed 8m 59s (remain 0m 31s) Loss: 0.0004(0.0277) Grad: 185.2994  LR: 0.00001550  \n",
      "Epoch: [1][2800/2860] Elapsed 9m 19s (remain 0m 11s) Loss: 0.0059(0.0275) Grad: 6190.0737  LR: 0.00001519  \n",
      "Epoch: [1][2859/2860] Elapsed 9m 31s (remain 0m 0s) Loss: 0.0300(0.0273) Grad: 18355.2871  LR: 0.00001500  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 3m 13s) Loss: 0.0145(0.0145) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 34s) Loss: 0.0022(0.0131) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0053(0.0161) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0072(0.0155) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.0044(0.0170) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0064(0.0168) \n",
      "EVAL: [600/715] Elapsed 0m 32s (remain 0m 6s) Loss: 0.0033(0.0169) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0002(0.0157) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0081(0.0155) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0273  avg_val_loss: 0.0155  time: 614s\n",
      "Epoch 1 - Score: 0.6990\n",
      "Epoch 1 - Save Best Score: 0.6990 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/2860] Elapsed 0m 0s (remain 19m 24s) Loss: 0.0172(0.0172) Grad: 22219.3574  LR: 0.00001500  \n",
      "Epoch: [2][100/2860] Elapsed 0m 20s (remain 9m 16s) Loss: 0.0114(0.0112) Grad: 29114.2734  LR: 0.00001468  \n",
      "Epoch: [2][200/2860] Elapsed 0m 40s (remain 8m 53s) Loss: 0.0011(0.0115) Grad: 13456.5596  LR: 0.00001435  \n",
      "Epoch: [2][300/2860] Elapsed 1m 0s (remain 8m 32s) Loss: 0.0672(0.0124) Grad: 84064.7031  LR: 0.00001402  \n",
      "Epoch: [2][400/2860] Elapsed 1m 20s (remain 8m 12s) Loss: 0.0008(0.0128) Grad: 4836.3472  LR: 0.00001368  \n",
      "Epoch: [2][500/2860] Elapsed 1m 40s (remain 7m 52s) Loss: 0.0156(0.0131) Grad: 45638.7344  LR: 0.00001334  \n",
      "Epoch: [2][600/2860] Elapsed 2m 0s (remain 7m 32s) Loss: 0.0424(0.0129) Grad: 131164.2500  LR: 0.00001299  \n",
      "Epoch: [2][700/2860] Elapsed 2m 20s (remain 7m 12s) Loss: 0.0020(0.0127) Grad: 7190.7695  LR: 0.00001264  \n",
      "Epoch: [2][800/2860] Elapsed 2m 40s (remain 6m 51s) Loss: 0.0009(0.0127) Grad: 7664.2510  LR: 0.00001228  \n",
      "Epoch: [2][900/2860] Elapsed 3m 0s (remain 6m 31s) Loss: 0.0052(0.0128) Grad: 20474.3906  LR: 0.00001192  \n",
      "Epoch: [2][1000/2860] Elapsed 3m 20s (remain 6m 11s) Loss: 0.0067(0.0129) Grad: 26506.9062  LR: 0.00001156  \n",
      "Epoch: [2][1100/2860] Elapsed 3m 40s (remain 5m 51s) Loss: 0.0052(0.0129) Grad: 20377.4062  LR: 0.00001120  \n",
      "Epoch: [2][1200/2860] Elapsed 4m 0s (remain 5m 31s) Loss: 0.0211(0.0127) Grad: 55332.3359  LR: 0.00001084  \n",
      "Epoch: [2][1300/2860] Elapsed 4m 20s (remain 5m 11s) Loss: 0.0002(0.0127) Grad: 896.8710  LR: 0.00001047  \n",
      "Epoch: [2][1400/2860] Elapsed 4m 40s (remain 4m 51s) Loss: 0.0006(0.0126) Grad: 5078.5347  LR: 0.00001011  \n",
      "Epoch: [2][1500/2860] Elapsed 4m 59s (remain 4m 31s) Loss: 0.0026(0.0127) Grad: 27501.3184  LR: 0.00000974  \n",
      "Epoch: [2][1600/2860] Elapsed 5m 19s (remain 4m 11s) Loss: 0.0071(0.0127) Grad: 7903.9302  LR: 0.00000937  \n",
      "Epoch: [2][1700/2860] Elapsed 5m 39s (remain 3m 51s) Loss: 0.0013(0.0128) Grad: 5176.1519  LR: 0.00000901  \n",
      "Epoch: [2][1800/2860] Elapsed 5m 59s (remain 3m 31s) Loss: 0.0206(0.0125) Grad: 28647.0352  LR: 0.00000865  \n",
      "Epoch: [2][1900/2860] Elapsed 6m 19s (remain 3m 11s) Loss: 0.0197(0.0125) Grad: 37933.6758  LR: 0.00000828  \n",
      "Epoch: [2][2000/2860] Elapsed 6m 39s (remain 2m 51s) Loss: 0.0052(0.0125) Grad: 4254.7417  LR: 0.00000792  \n",
      "Epoch: [2][2100/2860] Elapsed 6m 59s (remain 2m 31s) Loss: 0.0091(0.0125) Grad: 17064.2656  LR: 0.00000757  \n",
      "Epoch: [2][2200/2860] Elapsed 7m 19s (remain 2m 11s) Loss: 0.0006(0.0125) Grad: 3377.9426  LR: 0.00000721  \n",
      "Epoch: [2][2300/2860] Elapsed 7m 39s (remain 1m 51s) Loss: 0.0455(0.0126) Grad: 34207.1992  LR: 0.00000686  \n",
      "Epoch: [2][2400/2860] Elapsed 7m 59s (remain 1m 31s) Loss: 0.0233(0.0126) Grad: 22873.7109  LR: 0.00000652  \n",
      "Epoch: [2][2500/2860] Elapsed 8m 19s (remain 1m 11s) Loss: 0.0180(0.0126) Grad: 42125.4609  LR: 0.00000618  \n",
      "Epoch: [2][2600/2860] Elapsed 8m 39s (remain 0m 51s) Loss: 0.0286(0.0126) Grad: 28841.9043  LR: 0.00000584  \n",
      "Epoch: [2][2700/2860] Elapsed 8m 59s (remain 0m 31s) Loss: 0.0143(0.0127) Grad: 36647.7969  LR: 0.00000551  \n",
      "Epoch: [2][2800/2860] Elapsed 9m 19s (remain 0m 11s) Loss: 0.0166(0.0127) Grad: 20909.3320  LR: 0.00000519  \n",
      "Epoch: [2][2859/2860] Elapsed 9m 30s (remain 0m 0s) Loss: 0.0035(0.0128) Grad: 14001.0811  LR: 0.00000500  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 3m 33s) Loss: 0.0116(0.0116) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 34s) Loss: 0.0033(0.0127) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0030(0.0159) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0043(0.0158) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.0019(0.0176) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0012(0.0173) \n",
      "EVAL: [600/715] Elapsed 0m 32s (remain 0m 6s) Loss: 0.0034(0.0171) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0000(0.0159) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0089(0.0157) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0128  avg_val_loss: 0.0157  time: 613s\n",
      "Epoch 2 - Score: 0.7053\n",
      "Epoch 2 - Save Best Score: 0.7053 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/2860] Elapsed 0m 0s (remain 21m 20s) Loss: 0.0426(0.0426) Grad: 61365.9102  LR: 0.00000500  \n",
      "Epoch: [3][100/2860] Elapsed 0m 20s (remain 9m 17s) Loss: 0.0033(0.0106) Grad: 15165.4365  LR: 0.00000468  \n",
      "Epoch: [3][200/2860] Elapsed 0m 40s (remain 8m 54s) Loss: 0.0002(0.0103) Grad: 1033.1006  LR: 0.00000438  \n",
      "Epoch: [3][300/2860] Elapsed 1m 0s (remain 8m 33s) Loss: 0.0030(0.0117) Grad: 10927.0234  LR: 0.00000408  \n",
      "Epoch: [3][400/2860] Elapsed 1m 20s (remain 8m 12s) Loss: 0.0016(0.0112) Grad: 8635.7578  LR: 0.00000379  \n",
      "Epoch: [3][500/2860] Elapsed 1m 40s (remain 7m 52s) Loss: 0.1072(0.0106) Grad: 202598.5469  LR: 0.00000350  \n",
      "Epoch: [3][600/2860] Elapsed 2m 0s (remain 7m 32s) Loss: 0.0086(0.0108) Grad: 39753.7578  LR: 0.00000323  \n",
      "Epoch: [3][700/2860] Elapsed 2m 20s (remain 7m 12s) Loss: 0.0134(0.0108) Grad: 62397.9766  LR: 0.00000297  \n",
      "Epoch: [3][800/2860] Elapsed 2m 40s (remain 6m 52s) Loss: 0.0001(0.0109) Grad: 133.4473  LR: 0.00000271  \n",
      "Epoch: [3][900/2860] Elapsed 3m 0s (remain 6m 32s) Loss: 0.0297(0.0107) Grad: 40560.2344  LR: 0.00000246  \n",
      "Epoch: [3][1000/2860] Elapsed 3m 20s (remain 6m 12s) Loss: 0.0056(0.0105) Grad: 24923.1270  LR: 0.00000223  \n",
      "Epoch: [3][1100/2860] Elapsed 3m 40s (remain 5m 52s) Loss: 0.0192(0.0108) Grad: 100326.2812  LR: 0.00000200  \n",
      "Epoch: [3][1200/2860] Elapsed 4m 0s (remain 5m 32s) Loss: 0.0183(0.0105) Grad: 34822.6250  LR: 0.00000179  \n",
      "Epoch: [3][1300/2860] Elapsed 4m 20s (remain 5m 12s) Loss: 0.0239(0.0105) Grad: 64062.3594  LR: 0.00000159  \n",
      "Epoch: [3][1400/2860] Elapsed 4m 40s (remain 4m 52s) Loss: 0.0038(0.0106) Grad: 24192.1094  LR: 0.00000139  \n",
      "Epoch: [3][1500/2860] Elapsed 5m 0s (remain 4m 32s) Loss: 0.0075(0.0104) Grad: 24472.3379  LR: 0.00000121  \n",
      "Epoch: [3][1600/2860] Elapsed 5m 20s (remain 4m 12s) Loss: 0.0002(0.0105) Grad: 2238.2866  LR: 0.00000104  \n",
      "Epoch: [3][1700/2860] Elapsed 5m 40s (remain 3m 52s) Loss: 0.0023(0.0106) Grad: 10083.6719  LR: 0.00000089  \n",
      "Epoch: [3][1800/2860] Elapsed 6m 0s (remain 3m 32s) Loss: 0.0075(0.0105) Grad: 54438.5938  LR: 0.00000074  \n",
      "Epoch: [3][1900/2860] Elapsed 6m 20s (remain 3m 11s) Loss: 0.0561(0.0106) Grad: 126220.9922  LR: 0.00000061  \n",
      "Epoch: [3][2000/2860] Elapsed 6m 40s (remain 2m 51s) Loss: 0.0029(0.0106) Grad: 29317.6582  LR: 0.00000049  \n",
      "Epoch: [3][2100/2860] Elapsed 7m 0s (remain 2m 31s) Loss: 0.0087(0.0107) Grad: 76245.2734  LR: 0.00000038  \n",
      "Epoch: [3][2200/2860] Elapsed 7m 20s (remain 2m 11s) Loss: 0.0001(0.0106) Grad: 1323.3783  LR: 0.00000029  \n",
      "Epoch: [3][2300/2860] Elapsed 7m 40s (remain 1m 51s) Loss: 0.0013(0.0108) Grad: 23093.5234  LR: 0.00000021  \n",
      "Epoch: [3][2400/2860] Elapsed 8m 0s (remain 1m 31s) Loss: 0.0224(0.0108) Grad: 360338.2500  LR: 0.00000014  \n",
      "Epoch: [3][2500/2860] Elapsed 8m 20s (remain 1m 11s) Loss: 0.0047(0.0107) Grad: 137679.2812  LR: 0.00000009  \n",
      "Epoch: [3][2600/2860] Elapsed 8m 40s (remain 0m 51s) Loss: 0.0000(0.0107) Grad: 97.3600  LR: 0.00000004  \n",
      "Epoch: [3][2700/2860] Elapsed 9m 0s (remain 0m 31s) Loss: 0.0002(0.0107) Grad: 4939.9316  LR: 0.00000002  \n",
      "Epoch: [3][2800/2860] Elapsed 9m 20s (remain 0m 11s) Loss: 0.0002(0.0106) Grad: 4746.6299  LR: 0.00000000  \n",
      "Epoch: [3][2859/2860] Elapsed 9m 32s (remain 0m 0s) Loss: 0.0115(0.0107) Grad: 130026.9219  LR: 0.00000000  \n",
      "EVAL: [0/715] Elapsed 0m 0s (remain 3m 20s) Loss: 0.0148(0.0148) \n",
      "EVAL: [100/715] Elapsed 0m 5s (remain 0m 34s) Loss: 0.0046(0.0151) \n",
      "EVAL: [200/715] Elapsed 0m 11s (remain 0m 28s) Loss: 0.0023(0.0185) \n",
      "EVAL: [300/715] Elapsed 0m 16s (remain 0m 22s) Loss: 0.0018(0.0179) \n",
      "EVAL: [400/715] Elapsed 0m 22s (remain 0m 17s) Loss: 0.0010(0.0203) \n",
      "EVAL: [500/715] Elapsed 0m 27s (remain 0m 11s) Loss: 0.0012(0.0200) \n",
      "EVAL: [600/715] Elapsed 0m 33s (remain 0m 6s) Loss: 0.0016(0.0198) \n",
      "EVAL: [700/715] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0000(0.0184) \n",
      "EVAL: [714/715] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0099(0.0181) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0107  avg_val_loss: 0.0181  time: 615s\n",
      "Epoch 3 - Score: 0.7104\n",
      "Epoch 3 - Save Best Score: 0.7104 Model\n",
      "Saved logits to ./-kaggle-input-bio-clinicalbert_fold4_logits.npy\n",
      "========== fold: 4 result ==========\n",
      "Score: 0.7104\n",
      "========== CV ==========\n",
      "Score: 0.7075\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 61... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728ba6718382499e96656f7c714bcdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>█▂▁</td></tr><tr><td>[fold0] avg_val_loss</td><td>▁██</td></tr><tr><td>[fold0] epoch</td><td>▁▅█</td></tr><tr><td>[fold0] loss</td><td>▂▁▂▄▂█▂▁▅▂▂▁▁▁▁▂▁▂▃▁▁▆▃▂▁▁▁▁▁▁▁▂▁▂▁▂▁▁▁▃</td></tr><tr><td>[fold0] lr</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold0] score</td><td>▁▇█</td></tr><tr><td>[fold1] avg_train_loss</td><td>█▂▁</td></tr><tr><td>[fold1] avg_val_loss</td><td>▁▆█</td></tr><tr><td>[fold1] epoch</td><td>▁▅█</td></tr><tr><td>[fold1] loss</td><td>▄▄▂█▁▄▃▁▂▁▂▁▁▁▃▁▅▁▄▁▁▁▁▂▁▂▁▂▁▁▁▂▁▁▁▁▂▁▂▁</td></tr><tr><td>[fold1] lr</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold1] score</td><td>▁█▆</td></tr><tr><td>[fold2] avg_train_loss</td><td>█▂▁</td></tr><tr><td>[fold2] avg_val_loss</td><td>▁██</td></tr><tr><td>[fold2] epoch</td><td>▁▅█</td></tr><tr><td>[fold2] loss</td><td>▃▃▆▁▂▂█▂▅▃▁▁▆▁▃▁▁▃▂▃▁▂▁▁▂▂▁▂▁▁▁▁▂▂▁▁▁▁▁▂</td></tr><tr><td>[fold2] lr</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold2] score</td><td>▁▇█</td></tr><tr><td>[fold3] avg_train_loss</td><td>█▂▁</td></tr><tr><td>[fold3] avg_val_loss</td><td>▁▆█</td></tr><tr><td>[fold3] epoch</td><td>▁▅█</td></tr><tr><td>[fold3] loss</td><td>▃█▄▂▅▁▁▃▄▁▁▁▁▁▁▆▁▃▂▁▄▁▇▁▁▂▂▁▃▂▂▃▂▃▁▅▂▁▅▁</td></tr><tr><td>[fold3] lr</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold3] score</td><td>▁██</td></tr><tr><td>[fold4] avg_train_loss</td><td>█▂▁</td></tr><tr><td>[fold4] avg_val_loss</td><td>▁▁█</td></tr><tr><td>[fold4] epoch</td><td>▁▅█</td></tr><tr><td>[fold4] loss</td><td>▆▂▂▇▃▂▂▄▁▂▃▁▃▁▁▂▁▂▂▁▃▂▂▁▁▁▄▁▁█▂▃▄▂▁▁▄▁▂▁</td></tr><tr><td>[fold4] lr</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold4] score</td><td>▁▅█</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>0.01157</td></tr><tr><td>[fold0] avg_val_loss</td><td>0.01809</td></tr><tr><td>[fold0] epoch</td><td>3</td></tr><tr><td>[fold0] loss</td><td>0.0395</td></tr><tr><td>[fold0] lr</td><td>0.0</td></tr><tr><td>[fold0] score</td><td>0.70935</td></tr><tr><td>[fold1] avg_train_loss</td><td>0.0111</td></tr><tr><td>[fold1] avg_val_loss</td><td>0.01808</td></tr><tr><td>[fold1] epoch</td><td>3</td></tr><tr><td>[fold1] loss</td><td>0.01035</td></tr><tr><td>[fold1] lr</td><td>0.0</td></tr><tr><td>[fold1] score</td><td>0.70563</td></tr><tr><td>[fold2] avg_train_loss</td><td>0.01191</td></tr><tr><td>[fold2] avg_val_loss</td><td>0.01783</td></tr><tr><td>[fold2] epoch</td><td>3</td></tr><tr><td>[fold2] loss</td><td>0.07572</td></tr><tr><td>[fold2] lr</td><td>0.0</td></tr><tr><td>[fold2] score</td><td>0.70218</td></tr><tr><td>[fold3] avg_train_loss</td><td>0.01156</td></tr><tr><td>[fold3] avg_val_loss</td><td>0.01775</td></tr><tr><td>[fold3] epoch</td><td>3</td></tr><tr><td>[fold3] loss</td><td>0.00021</td></tr><tr><td>[fold3] lr</td><td>0.0</td></tr><tr><td>[fold3] score</td><td>0.70581</td></tr><tr><td>[fold4] avg_train_loss</td><td>0.01067</td></tr><tr><td>[fold4] avg_val_loss</td><td>0.01811</td></tr><tr><td>[fold4] epoch</td><td>3</td></tr><tr><td>[fold4] loss</td><td>0.0115</td></tr><tr><td>[fold4] lr</td><td>0.0</td></tr><tr><td>[fold4] score</td><td>0.71039</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">/kaggle/input/bio-clinicalbert</strong>: <a href=\"https://wandb.ai/anony-moose-234118703258025913/NBME-Public/runs/2ju4kja4?apiKey=52bf7cd93ad351d276f3e0fcdc3cd6b4a6347127\" target=\"_blank\">https://wandb.ai/anony-moose-234118703258025913/NBME-Public/runs/2ju4kja4?apiKey=52bf7cd93ad351d276f3e0fcdc3cd6b4a6347127</a><br/>\n",
       "Find logs at: <code>./wandb/run-20250528_071833-2ju4kja4/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = create_labels_for_scoring(oof_df)\n",
    "        predictions = oof_df[[i for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(oof_df['pn_history'].values, predictions, CFG.tokenizer)\n",
    "        results = get_results(char_probs, th=0.5)\n",
    "        preds = get_predictions(results)\n",
    "        score = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
    "        \n",
    "    if CFG.wandb:\n",
    "        wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 3075283,
     "sourceId": 33607,
     "sourceType": "competition"
    },
    {
     "datasetId": 2001834,
     "sourceId": 3503773,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 87864637,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 242061826,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30158,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9388.416412,
   "end_time": "2025-05-28T09:54:50.941568",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-28T07:18:22.525156",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0c82c3d817f04b1e839a715d7858e9ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a242cb59d6e541c99570c3001d8b7402",
       "placeholder": "​",
       "style": "IPY_MODEL_ad2e5e5fb15b475e9f09da9010b0a426",
       "value": " 143/143 [00:00&lt;00:00, 4692.20it/s]"
      }
     },
     "1c5dfc0c4f934d3c826ad7087e5433f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2538e315f307497bb83075bb9eababc0",
       "max": 143.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d9536586fe8946419a34ae3a80874fc4",
       "value": 143.0
      }
     },
     "2538e315f307497bb83075bb9eababc0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28a4281c92ee479db8a017d6338ee753": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "40a8b8ed9edc49a389c17ef2cb895d90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4389d00bdc514f50ba416f51acb13e4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7e8881782203480295a78910fc09e649",
       "max": 42146.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8332c113cfdf4f10bbed927a9daa9f0b",
       "value": 42146.0
      }
     },
     "56eb1b7643ce40178c721ed7d58aa795": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fd0fd8f07b034c2f96e36641a8d9c888",
        "IPY_MODEL_1c5dfc0c4f934d3c826ad7087e5433f1",
        "IPY_MODEL_0c82c3d817f04b1e839a715d7858e9ef"
       ],
       "layout": "IPY_MODEL_d65fbc78ef084090a53d4f630bd0a3c0"
      }
     },
     "592b1220ee5f4381a5336c5017c1f4d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "LabelModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "LabelModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "LabelView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_28a4281c92ee479db8a017d6338ee753",
       "placeholder": "​",
       "style": "IPY_MODEL_40a8b8ed9edc49a389c17ef2cb895d90",
       "value": " 0.08MB of 0.08MB uploaded (0.00MB deduped)\r"
      }
     },
     "6cc8e7af27b84deb881b67e501e38ec4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cf67c78f0be14e119ecdc112d3b960d7",
       "placeholder": "​",
       "style": "IPY_MODEL_c3703ea375784ddcacd9b0b0b1643cff",
       "value": "100%"
      }
     },
     "70dc8b90e59d4cf180bcd8fa2f65630b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "72482a7c27eb4566a1bb746c33730119": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "728ba6718382499e96656f7c714bcdfc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "VBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "VBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_592b1220ee5f4381a5336c5017c1f4d3",
        "IPY_MODEL_dafa8c5e04294efb93c1c1f1d36a669d"
       ],
       "layout": "IPY_MODEL_70dc8b90e59d4cf180bcd8fa2f65630b"
      }
     },
     "73be62ac4f3a490c96cad587ca95e0e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6cc8e7af27b84deb881b67e501e38ec4",
        "IPY_MODEL_4389d00bdc514f50ba416f51acb13e4f",
        "IPY_MODEL_d43bfda38dbb4251b9ddfa41ab2f12d2"
       ],
       "layout": "IPY_MODEL_bc4d9ec04f414fdfac171e061f020a88"
      }
     },
     "7e8881782203480295a78910fc09e649": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8332c113cfdf4f10bbed927a9daa9f0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8b8df781d3b645d4a9b3df342adbd765": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "93f1eca2a61148baa5441fe24092e0d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "981be6c622724f669d2249a82629ae62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a242cb59d6e541c99570c3001d8b7402": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a657260bdcfc411bacaed7b7e21b48b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad2e5e5fb15b475e9f09da9010b0a426": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bc4d9ec04f414fdfac171e061f020a88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c3703ea375784ddcacd9b0b0b1643cff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cf67c78f0be14e119ecdc112d3b960d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d43bfda38dbb4251b9ddfa41ab2f12d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a657260bdcfc411bacaed7b7e21b48b7",
       "placeholder": "​",
       "style": "IPY_MODEL_8b8df781d3b645d4a9b3df342adbd765",
       "value": " 42146/42146 [00:27&lt;00:00, 1623.89it/s]"
      }
     },
     "d65fbc78ef084090a53d4f630bd0a3c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d9536586fe8946419a34ae3a80874fc4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dafa8c5e04294efb93c1c1f1d36a669d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_72482a7c27eb4566a1bb746c33730119",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_93f1eca2a61148baa5441fe24092e0d8",
       "value": 1.0
      }
     },
     "edbb2a472e664b538eb0f8e3acce3609": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fd0fd8f07b034c2f96e36641a8d9c888": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_981be6c622724f669d2249a82629ae62",
       "placeholder": "​",
       "style": "IPY_MODEL_edbb2a472e664b538eb0f8e3acce3609",
       "value": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
