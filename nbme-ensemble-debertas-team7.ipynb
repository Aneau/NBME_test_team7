{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":33607,"databundleVersionId":3075283,"sourceType":"competition"},{"sourceId":3169336,"sourceType":"datasetVersion","datasetId":1926795},{"sourceId":3244804,"sourceType":"datasetVersion","datasetId":1966611},{"sourceId":3244848,"sourceType":"datasetVersion","datasetId":1966628},{"sourceId":3259238,"sourceType":"datasetVersion","datasetId":1974793},{"sourceId":87264998,"sourceType":"kernelVersion"}],"dockerImageVersionId":30163,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Ensemble of the three public Deberta notebooks. Scores 0.884 on the leaderboard (scoring takes 5+ hours).\n\nPlease upvote the original notebooks:\n\n**Deberta v3 large**\n\n[inference](https://www.kaggle.com/code/lunapandachan/nbme-thanh-s-infer-add-test)\n\n[original inference](https://www.kaggle.com/code/thanhns/deberta-v3-large-0-883-lb)\n\n[model](https://www.kaggle.com/datasets/thanhns/deberta-v3-large-5-folds-public)\n\n**Deberta v1 large**\n\n[inference](https://www.kaggle.com/code/manojprabhaakr/nbme-deberta-large-baseline-inference)\n\n[model](https://www.kaggle.com/datasets/manojprabhaakr/debertalarge)\n\n**Deberta v1 base**\n\n[train](https://www.kaggle.com/code/yasufuminakama/nbme-deberta-base-baseline-train)\n\n[inference](https://www.kaggle.com/code/yasufuminakama/nbme-deberta-base-baseline-inference)","metadata":{}},{"cell_type":"markdown","source":"**What is going on:**\nFirst I use the publicly available models to calculate the predictions_v3_l, predictions_v1_l and predictions_v3_b. These are lists of np.arrays. Each np.array corresponds to one patient note / feature number combination and represents probabilities that n-th letter in the patient note should be selected as belonging to the feature.\n\nThen in the very end I take these probabilities and for each patient note+feature number combine them in a simple linear combination:\n```\npredictions = []\nfor p1, p2, p3 in zip(predictions_v3_l, predictions_v1_l, predictions_v1_b):\n    predictions.append(w1*p1 + w2*p2 + w3*p3)\n```\nThe weights `w1,w2,w3` I got from playing with the out-of-fold results that comes with each trained model (this is in a separate notebook, private at the moment).\n\nWith the final \"probabilities\" (they can now actually go above one, so not really probabilities any more), I just get the results\n\n```\nresults = get_results(predictions)\n```\n\nand save them.\n\n**Why it takes five hours:**\nThe notebook evaluates all three models on the full test dataset (\\~2000 patient notes) and then combines the results. The large Debertas take about 2 hours each, the base one takes about 1 hour. Reason why this happens is because the models are huge, Kaggle GPU does not have much RAM and you need to use small batchsizes. It may be possible to increase the batchsizes here by a bit (~ factor two), but I did not look into it. That would speed things up a little. When you run the notebook, the models are only evaluated for the five notes in \"test.csv\" so it runs way quicker.","metadata":{}},{"cell_type":"markdown","source":"# Weights","metadata":{"execution":{"iopub.status.busy":"2022-04-01T01:01:55.256298Z","iopub.execute_input":"2022-04-01T01:01:55.256585Z","iopub.status.idle":"2022-04-01T01:01:55.275393Z","shell.execute_reply.started":"2022-04-01T01:01:55.256514Z","shell.execute_reply":"2022-04-01T01:01:55.274772Z"}}},{"cell_type":"code","source":"w1 = 0.5     # Deberta v3 large\nw2 = 0.4     # Deberta v1 large\nw3 = 0.18    # Deberta v1 base","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:49:34.477535Z","iopub.execute_input":"2025-05-27T08:49:34.477755Z","iopub.status.idle":"2025-05-27T08:49:34.513759Z","shell.execute_reply.started":"2025-05-27T08:49:34.477694Z","shell.execute_reply":"2025-05-27T08:49:34.513325Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    \n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-05-27T08:49:34.515023Z","iopub.execute_input":"2025-05-27T08:49:34.515494Z","iopub.status.idle":"2025-05-27T08:49:34.550082Z","shell.execute_reply.started":"2025-05-27T08:49:34.515457Z","shell.execute_reply":"2025-05-27T08:49:34.549609Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport ast\nimport sys\nimport copy\nimport json\nimport math\nimport string\nimport pickle\nimport random\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:49:34.550733Z","iopub.execute_input":"2025-05-27T08:49:34.550889Z","iopub.status.idle":"2025-05-27T08:49:38.920615Z","shell.execute_reply.started":"2025-05-27T08:49:34.550870Z","shell.execute_reply":"2025-05-27T08:49:38.919830Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Seed","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed=42):\n    '''\n    Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.\n    '''\n    random.seed(seed)\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        # When running on the CuDNN backend, two further options must be set\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:49:38.922613Z","iopub.execute_input":"2025-05-27T08:49:38.922875Z","iopub.status.idle":"2025-05-27T08:49:38.930961Z","shell.execute_reply.started":"2025-05-27T08:49:38.922839Z","shell.execute_reply":"2025-05-27T08:49:38.930426Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Helper functions for scoring","metadata":{}},{"cell_type":"code","source":"def micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    \n    return f1_score(truths, preds)\n\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n        \n    return binary\n\n\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n        \n    return micro_f1(bin_preds, bin_truths)","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:49:38.931755Z","iopub.execute_input":"2025-05-27T08:49:38.931971Z","iopub.status.idle":"2025-05-27T08:49:38.950595Z","shell.execute_reply.started":"2025-05-27T08:49:38.931943Z","shell.execute_reply":"2025-05-27T08:49:38.949972Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_labels_for_scoring(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        if len(location_list) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n        \n    return truths\n\n\ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n            \n    return results\n\n\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n        \n    return results\n\n\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n        \n    return predictions\n\n\ndef get_score(y_true, y_pred):\n    return span_micro_f1(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:49:38.951605Z","iopub.execute_input":"2025-05-27T08:49:38.951807Z","iopub.status.idle":"2025-05-27T08:49:38.971591Z","shell.execute_reply.started":"2025-05-27T08:49:38.951784Z","shell.execute_reply":"2025-05-27T08:49:38.970891Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"main_dir=\"../input/nbme-score-clinical-patient-notes/\"\n\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\n\n\ntest = pd.read_csv(main_dir+'test.csv')\nsubmission = pd.read_csv(main_dir+'sample_submission.csv')\nfeatures = pd.read_csv(main_dir+'features.csv')\npatient_notes = pd.read_csv(main_dir+'patient_notes.csv')\n\nfeatures = preprocess_features(features)\n\nprint(f\"test.shape: {test.shape}\")\ndisplay(test.head())\nprint(f\"features.shape: {features.shape}\")\ndisplay(features.head())\nprint(f\"patient_notes.shape: {patient_notes.shape}\")\ndisplay(patient_notes.head())","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:49:38.972680Z","iopub.execute_input":"2025-05-27T08:49:38.972984Z","iopub.status.idle":"2025-05-27T08:49:39.698504Z","shell.execute_reply.started":"2025-05-27T08:49:38.972959Z","shell.execute_reply":"2025-05-27T08:49:39.697786Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = test.merge(features, on=['feature_num', 'case_num'], how='left')\ntest = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:49:39.699578Z","iopub.execute_input":"2025-05-27T08:49:39.699832Z","iopub.status.idle":"2025-05-27T08:49:39.730387Z","shell.execute_reply.started":"2025-05-27T08:49:39.699787Z","shell.execute_reply":"2025-05-27T08:49:39.729716Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Deberta v3 large","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=\"../input/deberta-v3-large-5-folds-public/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=32\n    fc_dropout=0.2\n    max_len=354\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:49:39.731399Z","iopub.execute_input":"2025-05-27T08:49:39.731601Z","iopub.status.idle":"2025-05-27T08:49:39.735623Z","shell.execute_reply.started":"2025-05-27T08:49:39.731577Z","shell.execute_reply":"2025-05-27T08:49:39.735041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\ntokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\nCFG.tokenizer = tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:49:39.738518Z","iopub.execute_input":"2025-05-27T08:49:39.738841Z","iopub.status.idle":"2025-05-27T08:49:40.804730Z","shell.execute_reply.started":"2025-05-27T08:49:39.738798Z","shell.execute_reply":"2025-05-27T08:49:40.803903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_input(cfg, text, feature_text):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=CFG.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item])\n        \n        return inputs","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:49:40.805708Z","iopub.execute_input":"2025-05-27T08:49:40.805955Z","iopub.status.idle":"2025-05-27T08:49:40.812736Z","shell.execute_reply.started":"2025-05-27T08:49:40.805911Z","shell.execute_reply":"2025-05-27T08:49:40.811973Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ScoringModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        \n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        \n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:49:40.813887Z","iopub.execute_input":"2025-05-27T08:49:40.814124Z","iopub.status.idle":"2025-05-27T08:49:40.828157Z","shell.execute_reply.started":"2025-05-27T08:49:40.814099Z","shell.execute_reply":"2025-05-27T08:49:40.827376Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    \n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:49:40.829242Z","iopub.execute_input":"2025-05-27T08:49:40.829522Z","iopub.status.idle":"2025-05-27T08:49:40.841459Z","shell.execute_reply.started":"2025-05-27T08:49:40.829495Z","shell.execute_reply":"2025-05-27T08:49:40.840946Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = ScoringModel(CFG, config_path=CFG.config_path, pretrained=False)\n    \n    state = torch.load(CFG.path+f\"{CFG.model.split('/')[1]}_fold{fold}_best.pth\",\n                           map_location=torch.device('cpu'))\n       \n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs\n    gc.collect()\n    torch.cuda.empty_cache()\n    \npredictions_v3_l = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:49:40.842368Z","iopub.execute_input":"2025-05-27T08:49:40.842981Z","iopub.status.idle":"2025-05-27T08:51:46.391752Z","shell.execute_reply.started":"2025-05-27T08:49:40.842947Z","shell.execute_reply":"2025-05-27T08:51:46.389080Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Deberta large","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    path=\"../input/debertalarge/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-large\"\n    batch_size=24\n    fc_dropout=0.2\n    max_len=466\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:51:46.393165Z","iopub.execute_input":"2025-05-27T08:51:46.393556Z","iopub.status.idle":"2025-05-27T08:51:46.399593Z","shell.execute_reply.started":"2025-05-27T08:51:46.393514Z","shell.execute_reply":"2025-05-27T08:51:46.398978Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:51:46.400503Z","iopub.execute_input":"2025-05-27T08:51:46.400745Z","iopub.status.idle":"2025-05-27T08:51:46.563526Z","shell.execute_reply.started":"2025-05-27T08:51:46.400698Z","shell.execute_reply":"2025-05-27T08:51:46.562731Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:51:46.564739Z","iopub.execute_input":"2025-05-27T08:51:46.565003Z","iopub.status.idle":"2025-05-27T08:51:46.575241Z","shell.execute_reply.started":"2025-05-27T08:51:46.564968Z","shell.execute_reply":"2025-05-27T08:51:46.574624Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:51:46.576144Z","iopub.execute_input":"2025-05-27T08:51:46.576377Z","iopub.status.idle":"2025-05-27T08:51:46.590780Z","shell.execute_reply.started":"2025-05-27T08:51:46.576345Z","shell.execute_reply":"2025-05-27T08:51:46.590129Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_v1_l = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:51:46.591859Z","iopub.execute_input":"2025-05-27T08:51:46.592320Z","iopub.status.idle":"2025-05-27T08:53:16.165176Z","shell.execute_reply.started":"2025-05-27T08:51:46.592287Z","shell.execute_reply":"2025-05-27T08:53:16.164180Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Deberta base","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    num_workers=4\n    path=\"../input/nbme-deberta-base-baseline-train/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-base\"\n    batch_size=24\n    fc_dropout=0.2\n    max_len=466\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:53:16.166967Z","iopub.execute_input":"2025-05-27T08:53:16.167314Z","iopub.status.idle":"2025-05-27T08:53:16.176394Z","shell.execute_reply.started":"2025-05-27T08:53:16.167263Z","shell.execute_reply":"2025-05-27T08:53:16.175620Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:53:16.177541Z","iopub.execute_input":"2025-05-27T08:53:16.177750Z","iopub.status.idle":"2025-05-27T08:53:16.363536Z","shell.execute_reply.started":"2025-05-27T08:53:16.177726Z","shell.execute_reply":"2025-05-27T08:53:16.362984Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG.trn_fold:\n\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions_v1_b = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:53:16.364416Z","iopub.execute_input":"2025-05-27T08:53:16.364601Z","iopub.status.idle":"2025-05-27T08:54:18.050264Z","shell.execute_reply.started":"2025-05-27T08:53:16.364578Z","shell.execute_reply":"2025-05-27T08:54:18.049251Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Combine","metadata":{}},{"cell_type":"code","source":"predictions = []\nfor p1, p2, p3 in zip(predictions_v3_l, predictions_v1_l, predictions_v1_b):\n    predictions.append(w1*p1 + w2*p2 + w3*p3)","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:54:18.052131Z","iopub.execute_input":"2025-05-27T08:54:18.052374Z","iopub.status.idle":"2025-05-27T08:54:18.059016Z","shell.execute_reply.started":"2025-05-27T08:54:18.052339Z","shell.execute_reply":"2025-05-27T08:54:18.058161Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = get_results(predictions)\nsubmission['location'] = results\ndisplay(submission.head())\nsubmission[['id', 'location']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2025-05-27T08:54:18.060329Z","iopub.execute_input":"2025-05-27T08:54:18.060619Z","iopub.status.idle":"2025-05-27T08:54:18.088559Z","shell.execute_reply.started":"2025-05-27T08:54:18.060579Z","shell.execute_reply":"2025-05-27T08:54:18.087827Z"},"trusted":true},"outputs":[],"execution_count":null}]}